{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Clustering Using LDA\n",
    "\n",
    "The purpose of this tutorial is to give you a basic understanding of Latent Dirichlet Allocation (LDA) from http://ai.stanford.edu/~ang/papers/jair03-lda.pdf and use it to implement a simple downscaled topic clustering on the newsgroup dataset from scikit-learn.\n",
    "\n",
    "Section 1 will give some background to the mechanics and theory behind LDA. Section 2 will then tackle the task of implementing LDA to infer topics in documents based on their content. This section will provide you with skeleton code already written in Python 3 using the numpy, scipy, and scikit-learn libraries. \n",
    "\n",
    "If you do not have jupyter notebook installed then you probably aren't reading this, but see http://jupyter.readthedocs.io/en/latest/install.html\n",
    "\n",
    "If you do not have a python 3 kernel installed for jupyter notebook see https://ipython.readthedocs.io/en/latest/install/kernel_install.html or https://stackoverflow.com/questions/28831854/how-do-i-add-python3-kernel-to-jupyter-ipython\n",
    "\n",
    "If you do not have some of the libraries installed for your python 3 kernel, use the \"Kernel -> Conda packages\" dropdown menu in Jupyter if you used anaconda for your python 3 kernel, if not use the normal pip install commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Theory\n",
    "\n",
    "### Background and terminology\n",
    "\n",
    "Since we will be working in the setting of text corpora, we should clarify some of the terminology used in this setting:\n",
    "<ul>\n",
    "<li>A word is the basic unit of discrete data, defined to be an item from a vocabulary indexed by {1, . . . ,V}. \n",
    "<li> A document is a sequence of <i>N</i> words denoted by <b>w</b>=(<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>, . . . ,<i>w</i><sub>N</sub>), where <i>w</i><sub>n</sub> is the <i>n</i>th word in the sequence.</li>\n",
    "<li>A corpus is a collection of <i>M</i> documents denoted by $D$ ={<b>w</b><sub>1</sub>,<b>w</b><sub>2</sub>, . . . ,<b>w</b><sub>M</sub>}</li>\n",
    "</ul>\n",
    "\n",
    "It is important to note that LDA works in other domains besides text collections, but this is the setting in which we will use it.\n",
    "\n",
    "LDA is a generative probabilistic model that is used for modeling collections of discrete data. In our application we will be using it to model text corpora, or more specifically news group e-mails. The purpose of the model is to give us compact representations of the data in these collections, allowing us to process large collections while still retaining sufficient information to be able to perform for example classification and relevance measures. \n",
    "\n",
    "There have been several solutions for this type of information retrieval problem, such as the tf-idf (term frequency - inverse document frequency) scheme by Salton and McGill, 1983. This approach produces a term-by-document matrix X whose columns contain the tf-idf values for each of the documents in the corpus. This representation however did not provide significantly shortened representation of the corpora, or represent the inter- or intra- document statistics in a intuitive way. A step forward from this was given by LSI (latent semantic indexing) where singular value decomposition was used on the matrix X to offer a more compact representation. The authors of the method also argued that since the LSI features are linnear combinations of the basic tf-idf features, they incorporate some linguistical notions such as synonomy and polysemy.\n",
    "The first step to providing a generative model was the <i>probabilistic</i> LSI (pLSI), which uses mixture models to model each word in a document. The mixture components are the \"topics\" and represented as multinomial random variables, allowing different words in the document to be genereated by different topics. The compact representation for each document is then the list of numbers representing the mixing proportions for the fixed set of topics. The method however gives no generative probabilistic model for getting these numbers, causing the number of parameters in the model to grow linearly with the corpus size. Also, since there is no probabilistic model for the mixture components that represent a document, there is no clear way of assigning a probability to a document that is outside the training set.\n",
    "\n",
    "Both LSI and pLSI use the \"bag-of-words\" approach which assumes exchangeability within the words of the document as well as the documents themselves, meaning their order is of no importance. A theorem due to de Finetti (1990) states that any collection of exchangeable random variables has a representation as a mixture distribution—in general an infinite mixture. This means we must consider mixture models that capture the exchangeability of both documents and words if we wish to achieve exchangeable representations for them. It is this line of thinking that leads to LDA.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Theory Behind LDA\n",
    "\n",
    "As mentioned earlier, LDA is a generative probabilistic model for a corpus. It can be seen as a hierarchical Bayesian model with three levels: each document in a corpus is modeled as a finite random mixture over a latent set of topics, and each of these topics are characterized by a distribution of words. A graphical model for LDA using plate notation can be seen below:\n",
    "![title](imgs/LDAPlateGM.png)\n",
    "From here we can see the three levels of the model. $\\alpha$ and $\\beta$ and corpus level parameters, $\\theta$ is a document level parameter for the M documents in the corpus, and $z$ and $w$ are word level parameters for the N words in a document.\n",
    "\n",
    "The generative process according to LDA for each document <b>w</b> is then:\n",
    "<ol>\n",
    "<li>Choose N ∼Poisson(ξ)</li>\n",
    "<li>Choose $\\theta$∼Dir($\\alpha$)</li>\n",
    "<li>For each of the N words w<sub>n</sub>:\n",
    "<ol type=\"a\">\n",
    "    <li>Choose a topic z<sub>n</sub> ∼Multinomial($\\theta$).</li>\n",
    "    <li>Choose a word w<sub>n</sub> from p(w<sub>n</sub> |z<sub>n</sub>,$\\beta$), a multinomial probability conditioned on the topic z<sub>n</sub>.</li>\n",
    "    </ol></li>\n",
    "</ol>\n",
    "\n",
    "There are however some simplifications to these steps that we will utilize. First, we assume that the dimensionality of the Dirichlet distribution, and therefore the dimensionality for the topic variable $z$ is known and fixed, meaning we assume a fixed known number of topics, $k$. Furthermore, the probabilities for words ($w$) are parameterized by a $k \\times V$ matrix $\\beta$ which defines $p(w^j = 1| z^i = 1) = \\beta_{i,j}$, that we will estimate later and keep fixed. We also note that $N$ is independant of the other data generating variables $\\theta$ and <b>z</b> so we will ignore the Poisson assumption and set it to a known fixed value (the length of the document).  \n",
    "\n",
    "#### Dirichlet Distribution in LDA\n",
    "\n",
    "The probability distribution for a $k$-dimensional Dirichlet random variable $\\theta$ is defined as follows: \n",
    "\n",
    "<b>Eq. 1:</b>\n",
    "![Eq 1](imgs/LDAEq1.png \"Eq 1\")\n",
    "\n",
    "\n",
    "where $\\alpha$ is $k$-dimensional with all elements larger than 0 and $\\Gamma(x)$ is the Gamma function. The Dirichlet distribution has some advantegous advantageous qualities; it is in the exponential family, has finite dimensional sufficient statistics, and is conjugate to the multinomial distribution. These properties help us in running variational inference for the parameters later.\n",
    "\n",
    "We can now express the joint distribution of a topic mixture $\\theta$, a set of $N$ topics <b>z</b>, and\n",
    "a set of $N$ words <b>w</b> given the corpus level parameters $\\alpha,\\beta$ as:\n",
    "\n",
    "<b>Eq. 2:</b>\n",
    "![Eq. 2](imgs/LDAEq2.png)\n",
    "where the probability $p(z_n |\\theta)$ is simply $\\theta_i$ for the unique $i$ such that $z^i_n=1$. We can then obtain the marginal distribution over a document by integrating over $\\theta$ and summing over $z$:\n",
    "\n",
    "<b>Eq. 3:</b>\n",
    "![Eq. 3](imgs/LDAEq3.png)\n",
    "\n",
    "\n",
    "#### Comparison to other Latent Variable Models\n",
    "In order to get feeling for how LDA works and what highlights its strengths, it can be helpful to relate it to other related models:\n",
    "\n",
    "  a) Unigram Model\n",
    "\n",
    "  b) Mixture of Unigrams Model\n",
    "\n",
    "  c) pLSI Model\n",
    "  \n",
    "\n",
    "\n",
    "We will begin by examing the absolute simplest model, the unigram model: \n",
    "\n",
    "![Eq. 3](imgs/UniGramMdl.png)\n",
    "\n",
    "This method has no latent variables and instead states that each word in a document is independantly drawn from a single multinomial distribution as seen here:\n",
    "\n",
    "![Eq. 3](imgs/UniGramEq.png)\n",
    "\n",
    "\n",
    "A slighly more complex model is the mixture of unigrams:\n",
    "\n",
    "![Eq. 3](imgs/MixUniGramMdl.png)\n",
    "\n",
    "This model incorporates a discrete latent topic variable, $z$. Here, each document <b>w</b> is generated by first sampling the topic variable $z$, and then generating all words from a conditional probability on that choice:\n",
    "\n",
    "![Eq. 3](imgs/MixUniGramEq.png)\n",
    "\n",
    "This effectively limits the modeling of words in a document to only being representative of one topic. The LDA model on the other hand allows for documents to exhibit multiple topics with different mixtures.\n",
    "\n",
    "Finally we have the pLSI model which we mentioned earlier. It was a relatively popular model around the time that LDA was proposed, and is the model with highest generative capabilities of these three mentioned. \n",
    "\n",
    "![Eq. 3](imgs/PISLMdl.png)\n",
    "\n",
    "pLSI proposes that each word is conditionally independant a \"document label\", $d$, given an unobserved topic $z$:\n",
    "\n",
    "![Eq. 3](imgs/PISLEq.png)\n",
    "\n",
    "This proposal aims to soften the constraint of having each document modeled as being generated from only one topic, as it is in the mixture of unigrams approach. It does so by incorporating the probability, $p(z | d)$ for a certain document $d$ as the mixture of topics that document. A true generative model cannot be created for this mixture however; as d is only a dummy index to the documents pISL was trained with, meaning it is a multinomial random variable with the same amount of possible values as training documents. This leads to the method only learning the topic mixtures, $p(z | d)$, for documents it has already seen, so there is no natural way to assign probability to an unseen document with it.\n",
    "Another problem is that to model $k$ topics with pLSI you need K multinomial distributions with vocabulary size $V$ and $M$ mixtures over the hidden topics $k$ for each training document, resulting in $kV + kM$ parameters. Not only does this not scale well but it is also prone to overfitting.\n",
    "\n",
    "LDA however treats the topic mixture weights as a $k$-parameter hidden variable, meaning the amount of parameters does not scale linnearly with the number training documents, and the generative model can still be used even with unseen documents.\n",
    "\n",
    "We can see these differences geometrically as well if we examine the distribution over words as a $V$-1 dimensional on a vocabulary of size $V$ with another $k$-1 dimensional simplex spanning $k$ topics. We can set $V$ and $k$ to 3 for simplicity (3 words gives a two-dimensional triangle):\n",
    "\n",
    "![title](imgs/UnigramSampling.png)\n",
    "\n",
    "How this distribution is spread out and how it uses the topics distribution differs among the methods. The mixture of unigrams method pics a random point on the word simplex that corresponds to one of the topic simplex vertices k, and draws all the words for a document from the distribution corresponding to that point. pLSI assumes that all words in training documents belong to a single randomly chosen topic. The topics are drawn from a document-specific distribution, meaning each document has a topic distribution that sits on the topic simplex. The training documents then give an empirical distribution (with the marked 'x's) over the topic simplex. LDA instead models that <b>each word</b> in a document is drawn from a randomly chosen topic that is sampled from a distribution governed by a random parameter. Since this parameter is sampled once per document, it gives a smooth probability distribution over the topic simplex (the circular topology markers). \n",
    "\n",
    "Now that we know how LDA compares with other methods, lets take a look at how to do inference in LDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and Parameter Estimation with LDA\n",
    "\n",
    "The main inference problem we will be interested in solving is the posterior distribution of the latent variables given a document, which would allow us to infer the topics associated with the document. This is given by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\theta, \\mathbf{z} \\mid \\mathbf{w}, \\alpha, \\beta) = \\frac{p( \\theta, \\mathbf{z}, \\mathbf{w} \\mid \\alpha, \\beta)}{p(\\mathbf{w} \\mid \\alpha, \\beta)} \n",
    "\\end{equation}\n",
    "\n",
    "However, to compute the normalizing denominator we would rewrite equation 3 using equation 1 and $p(z_n \\mid\\theta)=\\theta_i$ and then integrate, resulting in:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\mathbf{w} \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\sum_i\\alpha_i)}{\\prod_i\\Gamma(\\alpha_i)}\\int\\Bigg(\\prod_{i=1}^k\\theta_i^{\\alpha_i-1}\\Bigg)\\Bigg(\\prod_{n=1}^N\\sum_{i=1}^k\\prod_{j=1}^V(\\theta_i\\beta_{ij})^{w_n^j}\\Bigg)d\\theta\n",
    "\\end{equation}\n",
    "\n",
    "Unfortunately, this expression is intractable due to the coupling of $\\theta$ and $\\beta$ in the summation over topics. We can however solve this problem approximately using variational inference methods. \n",
    "\n",
    "#### Variational Inference for LDA\n",
    "\n",
    "It is possible to use several VI methods for LDA, including La Place approximation, variational approximation, and MCMC methods. In our case, we will be using the convexity-based variational approximation that was mentioned in  Olga's tutorial. From there we learned that in this VI we attempt to reformulate/simplify the original graphical model by removing some dependencies and introducing free variational parameters. This leads to a family of distributions dependant on these variational parameters which form a lower bound on the log likelyhood. We then aim to find the parameter values that give the tightest lower bound. \n",
    "\n",
    "In our case, the problematic dependancy is between $\\theta$ and $\\beta$ which is introduced by the edges between $\\theta, \\mathbf{z}$ and $\\mathbf{w}$ (remember w is a 'collision' node and is observed). If we simplify our model by removing these edges along with the <b>w</b> node, and introduce two variational parameters $\\gamma$ and $\\phi$ which give a family of distributions over the remaining latent variables, we are left with the graphical model shown on the right in the figure below:\n",
    "\n",
    "![LDA VI](imgs/LDAVIGM.png \"GM for the VI used for our LDA\")\n",
    "\n",
    "This results in the following distribution over the latent variables:\n",
    "\n",
    "\\begin{equation}\n",
    "p( \\theta, \\mathbf{z} \\mid \\gamma, \\phi) = q(\\theta\\mid\\gamma)\\prod_{n=1}^Nq(z_n\\mid\\phi_n)\n",
    "\\end{equation}\n",
    "\n",
    "where the new Dirichlet parameters $\\gamma$ and the multinomial parameters $\\phi$ are the free variational parameters. Now having simplified our graphical model, we need to find the optimal values for the variational parameters ($\\gamma^*, \\phi^*$). From Olga's tutorial we know that this is equivalent to finding the values which minimize the KL divergence between the simplified distribution and the true posterior distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "( \\gamma^*, \\phi^*) = \\arg\\!\\min_{(\\gamma,\\phi)}D\\big(q( \\theta, \\mathbf{z} \\mid \\gamma, \\phi) \\mid\\mid p( \\theta, \\mathbf{z} \\mid \\mathbf{w},\\alpha, \\beta\\big)\n",
    "\\end{equation}\n",
    "\n",
    "We do this by setting the derivatives of the KL divergence to zero w.r.t $\\gamma$ and $\\phi$ we get the following update equations for the parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_{ni} \\propto \\beta_{iw_n}\\mathrm{exp}\\big\\lbrace\\mathrm{E}_q\\big[log(\\theta_i)\\mid\\gamma\\big]\\big\\rbrace = \\beta_{iw_n}\\mathrm{exp}\\big\\lbrace\\Psi(\\gamma_i)\\big\\rbrace\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\gamma_i = \\alpha_i + \\sum_{n=1}^N\\phi_{ni}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Psi$ is the digamma function (first derivative of log $\\Gamma$). It is important to note that these update equations derived from the KL divergence are dependant on a certain choice of <b>w</b>. This means that the shown approximation for the variational parameters is only valid for one set of words, and must therefore be calculated for each document when we use them later on. \n",
    "\n",
    "We must also find a way of estimating the $\\beta$ matrix, as it is used in the approximations for the variational parameters. The log likelihood of the data given $\\beta$ and $\\alpha$ is intractable as we saw at the end of the previous section. However, it is possible to implement a variational EM procedure that gives us an approximation of the best value for $\\beta$ by first maximizing a lower bound for the optimal variational parameters $\\gamma^*,\\phi^*$, then maximizing the lower bound w.r.t $\\beta$ with the previously acquired variational parameters. Essentially we will iterate the following steps until a sufficient level of convergence:\n",
    "<ol>\n",
    "<li>(E-step) Find the optimizing values of the variational parameters {$\\gamma^∗\n",
    "_d,\\phi^∗_d : d\\in D$},for each document as described earlier.</li>\n",
    "<li>(M-step) Maximize the resulting lower bound on the log likelihood w.r.t $\\beta$ using:\n",
    "\\begin{equation}\n",
    "\\beta_{ij}\\propto\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\phi^*_{dni}w^j_{dn}\n",
    "\\end{equation}\n",
    "as well as maximize the resulting lower bound on the log likelihood w.r.t $\\alpha$ (this will be given to you).\n",
    "</ol>\n",
    "\n",
    "In laymans terms, what we are essentially doing in the E-step is finding out \"How prevalent are topics in the document across its words?\". In the M-step we then ask \"How prevalent are specific words across topics?\". By using the answer from one question as a starting point for the other, we iteratively gain the answer to both. \n",
    "\n",
    "<span style=\"color:red\">For proof for the update equations, see appendix of http://ai.stanford.edu/~ang/papers/jair03-lda.pdf</span>. This appendix also includes the derivation of the Newton-Raphson based method for updating $\\alpha$.  \n",
    "\n",
    "We have now seen the basic intuition behind LDA, and gone through methods for running inference based on the LDA model. In the next section we will put this knowledge in to practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Implementation\n",
    "\n",
    "The goal of this task is to use LDA to create topics newsgroup documents, and infer the topic that new documents would belong to. In this setting, our document corpus is the Newsgroup dataset from scikit-learn, and a document is a certain document/e-mail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "First we will load the dataset we will use for training and testing. We will simplify the example from the original paper to only do clustering for 4 topics, and only use 250 documents with a vocabulary of 750 words. While this does have an effect on the accuracy and performance of the algorithm, it's more important for you to be able to run the code in a managable amount of time. The documents I have chosen come from 4 different categories; \"Christian Religion\", \"Hockey\", \"Space\" and \"Cars\". This means that we have slightly unrealistic prior knowledge by assuming the exact correct number of topics, but don't worry there are bonus assignments in the end where you can play around with the number of topics. I have already compiled and done some preprocessing on the documents, as well as built the vocabulary dataset as pickle files. Run the code in the cell below and double check that you get the output \"found 200 training and 50 test documents, with a vocabulary of 750 words\". Do not worry if you get a warning regarding the version of CountVectorizer which is used for handling the vocabulary of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  200 training and  50  test documents, with a vocabulary of  750  words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mangemange\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.18.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle as pickle\n",
    "\n",
    "vectorizer = pickle.load(open(\"Data/vectorizerNews.p\", \"rb\"))\n",
    "trainDocs = pickle.load(open(\"Data/trainDocsNews.p\", \"rb\"))\n",
    "testDocs = pickle.load(open(\"Data/testDocsNews.p\", \"rb\"))\n",
    "\n",
    "#also load the original docs that aren't pre-processed for viewing later\n",
    "origTrainDocs = pickle.load(open(\"Data/trainDocsNewsOrig.p\", \"rb\"))\n",
    "origTestDocs = pickle.load(open(\"Data/testDocsNewsOrig.p\", \"rb\"))\n",
    "\n",
    "print(\"Found \", len(trainDocs), \"training and \", len(testDocs), \" test documents, with a vocabulary of \", len(vectorizer.get_feature_names()), \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation\n",
    "\n",
    "We must now find the optimal values for the variational parameters, as well as the values for $\\alpha$ and the $\\beta$ matrix that were introduced in the variational inference section. In order to follow the instructions given in the VI section we will need to do some setup first, so run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import scipy.special as special\n",
    "import scipy.optimize \n",
    "import time\n",
    "\n",
    "#diGamma func from scipy, use this in your code!\n",
    "diGamma = special.digamma\n",
    "\n",
    "#Function definitions for maximizing the VI parameters. This will later be completed by you.\n",
    "def maxVIParam(phi, gamma, B, alpha, M, k, Wd, eta,verboseNum=-1):\n",
    "    #print(\"in maxVIParam\")\n",
    "    for d in range(M):\n",
    "        if(d==verboseNum):\n",
    "            verbose=True\n",
    "        else:\n",
    "            verbose=False\n",
    "            \n",
    "        N = len(Wd[d])\n",
    "        thisB = np.array([[B[i][Wd[d][n]] for n in range(N)] for i in range(k)])\n",
    "        #Initialization of vars, as shown in E-step.\n",
    "        phi[d] = np.ones((N,k))*1.0/k\n",
    "        gamma[d] = np.ones(k)*(N/k) + alpha\n",
    "        \n",
    "        converged = False\n",
    "        j = 0 \n",
    "        iter=0\n",
    "        \n",
    "        while(not(converged)):\n",
    "            #YOUR CODE FOR THE E-STEP HERE\n",
    "            # UPDATE OF PHI:\n",
    "            \n",
    "            phi[d] = thisB.T*np.exp(diGamma(gamma[d][None,:]))\n",
    "            normPhi = np.sum(phi[d],axis=(1)) # Sum over all (columns) i (topics) => Nx1\n",
    "            \n",
    "            if(np.min(normPhi)==0):\n",
    "                print(\"trying to divide with 0: \")\n",
    "                print(normPhi)\n",
    "                problemIdx = np.array(range(len(normPhi)))[np.where(normPhi==0)]\n",
    "                for i in problemIdx:\n",
    "                    print(phi[d][i,:])\n",
    "                \n",
    "            \n",
    "            phi[d] /= normPhi[:,None]\n",
    "\n",
    "            # UPDATE OF GAMMA:\n",
    "            \n",
    "            oldGamma = gamma[d]\n",
    "            gamma[d] = alpha + np.sum(phi[d],axis=(0)) # Sum over all (rows) n (words) => 1xk\n",
    "            converged = (max(np.abs(oldGamma-gamma[d])) < eta)\n",
    "            iter+=1\n",
    "    return gamma, phi\n",
    "\n",
    "#Function definitions for maximizing the B parameter. This will later be completed by you.\n",
    "def MaxB(B, phi, k, V, M, Wd):\n",
    "    \n",
    "    #YOUR CODE FOR THE M-STEP HERE\n",
    "    beta = np.zeros((k,V))\n",
    "\n",
    "    for d in range(M):\n",
    "        for w in range(len(Wd[d])):\n",
    "            beta[:,Wd[d][w]] += phi[d][w,:]\n",
    "    \n",
    "    beta /= np.sum(beta,axis=(1))[:,None]\n",
    "    return beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Here are the functions needed for updating the alpha parameter, as shown in the start of appendix A.4.2.\n",
    "These are all provided for you as it is just plugging in the definition for the gradient and hessian into the \n",
    "Newton-Raphson based method to find a stationary point using SciPy. Feel free to take a look at the appendix to\n",
    "see where these values come from.'''\n",
    "\n",
    "#value of Likelihood(gamma,phi,alpha,beta) function w.r.t. alpha terms (see start of appendix A.4.2) \n",
    "def L_alpha_val(a):\n",
    "    val = 0\n",
    "    M = len(gamma)\n",
    "    k = len(a)\n",
    "    for d in range(M):\n",
    "        val += (np.log(scipy.special.gamma(np.sum(a))) - np.sum([np.log(scipy.special.gamma(a[i])) for i in range(k)]) + np.sum([((a[i] -1)*(diGamma(gamma[d][i]) - diGamma(np.sum(gamma[d])))) for i in range(k)]))\n",
    "\n",
    "    return -val\n",
    "\n",
    "#value of the derivative of above func w.r.t. alpha_i (2nd eq of appendix A.4.2) \n",
    "def L_alpha_der(a):\n",
    "    M = len(gamma)\n",
    "    k = len(a)\n",
    "    der = np.array(\n",
    "    [(M*(diGamma(np.sum(a)) - diGamma(a[i])) + np.sum([diGamma(gamma[d][i]) - diGamma(np.sum(gamma[d])) for d in range(M)])) for i in range(k)]\n",
    "    )\n",
    "    return -der\n",
    "\n",
    "def L_alpha_hess(a):\n",
    "    hess = np.zeros((len(a),len(a)))\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a)):\n",
    "            k_delta = 1 if i == j else 0\n",
    "            hess[i,j] = k_delta*M*scipy.special.polygamma(1,a[i]) - scipy.special.polygamma(1,np.sum(a))\n",
    "    return -hess\n",
    "\n",
    "def MaxA(a):\n",
    "    res = scipy.optimize.minimize(L_alpha_val, a, method='Newton-CG',\n",
    "        jac=L_alpha_der, hess=L_alpha_hess,\n",
    "        options={'xtol': 1e-8, 'disp': False})\n",
    "\n",
    "    print(res.x)\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, we can now initialize the required parameters and define the skeleton of our loop for the parameter estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on iter:  0\n",
      "B max diff:  0.022841456494436994\n",
      "[1.74377368 1.64505421 1.67258949 1.43296063]\n",
      "iter took:  5.092540740966797\n",
      "new alpha:  [1.74377368 1.64505421 1.67258949 1.43296063]\n",
      "on iter:  1\n",
      "B max diff:  0.011264990630473336\n",
      "[1.39694089 1.30137507 1.32298266 1.10519272]\n",
      "iter took:  2.3868567943573\n",
      "new alpha:  [1.39694089 1.30137507 1.32298266 1.10519272]\n",
      "on iter:  2\n",
      "B max diff:  0.008006439067811608\n",
      "[0.87808522 0.8287542  0.82527307 0.6636437 ]\n",
      "iter took:  1.8482789993286133\n",
      "new alpha:  [0.87808522 0.8287542  0.82527307 0.6636437 ]\n",
      "on iter:  3\n",
      "B max diff:  0.0035607173597962463\n",
      "[0.57665212 0.55139347 0.53144578 0.42366697]\n",
      "iter took:  1.4438982009887695\n",
      "new alpha:  [0.57665212 0.55139347 0.53144578 0.42366697]\n",
      "on iter:  4\n",
      "B max diff:  0.002671915542849963\n",
      "[0.41748071 0.40087963 0.36313402 0.2950996 ]\n",
      "iter took:  1.258800983428955\n",
      "new alpha:  [0.41748071 0.40087963 0.36313402 0.2950996 ]\n",
      "on iter:  5\n",
      "B max diff:  0.002856420457206097\n",
      "[0.31767505 0.30678614 0.25823063 0.21604141]\n",
      "iter took:  1.3473789691925049\n",
      "new alpha:  [0.31767505 0.30678614 0.25823063 0.21604141]\n",
      "on iter:  6\n",
      "B max diff:  0.0022078976627893485\n",
      "[0.24959088 0.24147082 0.19519836 0.16859368]\n",
      "iter took:  1.481496810913086\n",
      "new alpha:  [0.24959088 0.24147082 0.19519836 0.16859368]\n",
      "on iter:  7\n",
      "B max diff:  0.0011698146441232382\n",
      "[0.20610838 0.19464632 0.15813806 0.13994698]\n",
      "iter took:  1.3744409084320068\n",
      "new alpha:  [0.20610838 0.19464632 0.15813806 0.13994698]\n",
      "on iter:  8\n",
      "B max diff:  0.0010921443202164083\n",
      "[0.17988573 0.16617418 0.13422241 0.12123301]\n",
      "iter took:  1.0169661045074463\n",
      "new alpha:  [0.17988573 0.16617418 0.13422241 0.12123301]\n",
      "on iter:  9\n",
      "B max diff:  0.0014958422936218858\n",
      "[0.16327898 0.14632224 0.11609849 0.10865143]\n",
      "iter took:  1.1577098369598389\n",
      "new alpha:  [0.16327898 0.14632224 0.11609849 0.10865143]\n",
      "on iter:  10\n",
      "B max diff:  0.0015473779914924497\n",
      "[0.15185579 0.13190642 0.10457424 0.0991113 ]\n",
      "iter took:  1.0143749713897705\n",
      "new alpha:  [0.15185579 0.13190642 0.10457424 0.0991113 ]\n",
      "on iter:  11\n",
      "B max diff:  0.0021930781295628025\n",
      "[0.14087396 0.12031733 0.0969347  0.09196436]\n",
      "iter took:  1.016791582107544\n",
      "new alpha:  [0.14087396 0.12031733 0.0969347  0.09196436]\n",
      "on iter:  12\n",
      "B max diff:  0.002280309646305197\n",
      "[0.13277878 0.11285152 0.09164517 0.08692661]\n",
      "iter took:  0.925523042678833\n",
      "new alpha:  [0.13277878 0.11285152 0.09164517 0.08692661]\n",
      "on iter:  13\n",
      "B max diff:  0.002687775615251242\n",
      "[0.12661079 0.10706191 0.08783119 0.08372343]\n",
      "iter took:  0.9194629192352295\n",
      "new alpha:  [0.12661079 0.10706191 0.08783119 0.08372343]\n",
      "on iter:  14\n",
      "B max diff:  0.0020746499307119394\n",
      "[0.12161788 0.10176065 0.0845014  0.08116909]\n",
      "iter took:  0.896601676940918\n",
      "new alpha:  [0.12161788 0.10176065 0.0845014  0.08116909]\n",
      "on iter:  15\n",
      "B max diff:  0.0012523829228732498\n",
      "[0.11789901 0.09578069 0.0819216  0.07901663]\n",
      "iter took:  0.7511937618255615\n",
      "new alpha:  [0.11789901 0.09578069 0.0819216  0.07901663]\n",
      "on iter:  16\n",
      "B max diff:  0.0006973567480253393\n",
      "[0.11509445 0.0924861  0.07951983 0.07684222]\n",
      "iter took:  2.1056039333343506\n",
      "new alpha:  [0.11509445 0.0924861  0.07951983 0.07684222]\n",
      "on iter:  17\n",
      "B max diff:  0.0004832410653564691\n",
      "[0.11297547 0.09071273 0.07735346 0.07520981]\n",
      "iter took:  0.7565798759460449\n",
      "new alpha:  [0.11297547 0.09071273 0.07735346 0.07520981]\n",
      "on iter:  18\n",
      "B max diff:  0.00045828614553184224\n",
      "[0.11061389 0.0892248  0.07493142 0.07387987]\n",
      "iter took:  0.7343583106994629\n",
      "new alpha:  [0.11061389 0.0892248  0.07493142 0.07387987]\n",
      "on iter:  19\n",
      "B max diff:  0.00045839640057841096\n",
      "[0.10823785 0.08792737 0.07324437 0.0727748 ]\n",
      "iter took:  0.6459832191467285\n",
      "new alpha:  [0.10823785 0.08792737 0.07324437 0.0727748 ]\n",
      "on iter:  20\n",
      "B max diff:  0.00044282175766642605\n",
      "[0.10662414 0.08682857 0.07204963 0.07188105]\n",
      "iter took:  0.6754891872406006\n",
      "new alpha:  [0.10662414 0.08682857 0.07204963 0.07188105]\n",
      "on iter:  21\n",
      "B max diff:  0.0005523671990412258\n",
      "[0.10548008 0.08591045 0.07119096 0.07116559]\n",
      "iter took:  0.648266077041626\n",
      "new alpha:  [0.10548008 0.08591045 0.07119096 0.07116559]\n",
      "on iter:  22\n",
      "B max diff:  0.0005110227909156887\n",
      "[0.10464241 0.08515198 0.07056568 0.0705938 ]\n",
      "iter took:  0.614372730255127\n",
      "new alpha:  [0.10464241 0.08515198 0.07056568 0.0705938 ]\n",
      "on iter:  23\n",
      "B max diff:  0.00045894448554517457\n",
      "[0.10394718 0.08400015 0.07007137 0.07009976]\n",
      "iter took:  0.8098795413970947\n",
      "new alpha:  [0.10394718 0.08400015 0.07007137 0.07009976]\n",
      "on iter:  24\n",
      "B max diff:  0.0007405251137901957\n",
      "[0.10331553 0.08316487 0.06930713 0.06964607]\n",
      "iter took:  0.6748061180114746\n",
      "new alpha:  [0.10331553 0.08316487 0.06930713 0.06964607]\n",
      "on iter:  25\n",
      "B max diff:  0.001984842858647706\n",
      "[0.10261098 0.08244743 0.06868742 0.0680512 ]\n",
      "iter took:  0.6361956596374512\n",
      "new alpha:  [0.10261098 0.08244743 0.06868742 0.0680512 ]\n",
      "on iter:  26\n",
      "B max diff:  0.0003545838206049487\n",
      "[0.10194713 0.08182976 0.06818906 0.06696292]\n",
      "iter took:  0.751741886138916\n",
      "new alpha:  [0.10194713 0.08182976 0.06818906 0.06696292]\n",
      "on iter:  27\n",
      "B max diff:  0.000428515659274883\n",
      "[0.10136111 0.08130367 0.06778788 0.06619941]\n",
      "iter took:  0.8676388263702393\n",
      "new alpha:  [0.10136111 0.08130367 0.06778788 0.06619941]\n",
      "on iter:  28\n",
      "B max diff:  0.0004127673808342957\n",
      "[0.10085721 0.0808602  0.06746233 0.06565031]\n",
      "iter took:  1.370872974395752\n",
      "new alpha:  [0.10085721 0.0808602  0.06746233 0.06565031]\n",
      "on iter:  29\n",
      "B max diff:  0.000347968289668162\n",
      "[0.10039115 0.08045377 0.06683709 0.06522459]\n",
      "iter took:  0.7721908092498779\n",
      "new alpha:  [0.10039115 0.08045377 0.06683709 0.06522459]\n",
      "on iter:  30\n",
      "B max diff:  0.00043768522510776787\n",
      "[0.09993255 0.07963432 0.0663615  0.06486386]\n",
      "iter took:  0.7319750785827637\n",
      "new alpha:  [0.09993255 0.07963432 0.0663615  0.06486386]\n",
      "on iter:  31\n",
      "B max diff:  0.0004967792567792837\n",
      "[0.09946335 0.07856932 0.06596542 0.06453569]\n",
      "iter took:  0.5187873840332031\n",
      "new alpha:  [0.09946335 0.07856932 0.06596542 0.06453569]\n",
      "on iter:  32\n",
      "B max diff:  0.0004843849446064971\n",
      "[0.09904116 0.07784629 0.06563912 0.06425173]\n",
      "iter took:  0.5797836780548096\n",
      "new alpha:  [0.09904116 0.07784629 0.06563912 0.06425173]\n",
      "on iter:  33\n",
      "B max diff:  0.00039199765996195774\n",
      "[0.09855962 0.07647262 0.06531745 0.06395883]\n",
      "iter took:  0.5976433753967285\n",
      "new alpha:  [0.09855962 0.07647262 0.06531745 0.06395883]\n",
      "on iter:  34\n",
      "B max diff:  0.000309495779081776\n",
      "[0.09809316 0.07557677 0.06502401 0.06368625]\n",
      "iter took:  1.7358124256134033\n",
      "new alpha:  [0.09809316 0.07557677 0.06502401 0.06368625]\n",
      "on iter:  35\n",
      "B max diff:  0.00023821071496005035\n",
      "[0.09708932 0.07492073 0.06473006 0.06341121]\n",
      "iter took:  0.5310773849487305\n",
      "new alpha:  [0.09708932 0.07492073 0.06473006 0.06341121]\n",
      "on iter:  36\n",
      "B max diff:  0.0004744122306724769\n",
      "[0.09644151 0.07443662 0.06446122 0.06316269]\n",
      "iter took:  0.6528263092041016\n",
      "new alpha:  [0.09644151 0.07443662 0.06446122 0.06316269]\n",
      "on iter:  37\n",
      "B max diff:  0.0004729027231566715\n",
      "[0.09599618 0.07407466 0.06422279 0.0629495 ]\n",
      "iter took:  0.6152639389038086\n",
      "new alpha:  [0.09599618 0.07407466 0.06422279 0.0629495 ]\n",
      "on iter:  38\n",
      "B max diff:  0.0002986910454253748\n",
      "[0.09502849 0.0737206  0.06361932 0.06271372]\n",
      "iter took:  0.6573338508605957\n",
      "new alpha:  [0.09502849 0.0737206  0.06361932 0.06271372]\n",
      "on iter:  39\n",
      "B max diff:  0.0005101411795643031\n",
      "[0.0944005  0.07340601 0.06316422 0.06249067]\n",
      "iter took:  0.652334451675415\n",
      "new alpha:  [0.0944005  0.07340601 0.06316422 0.06249067]\n",
      "on iter:  40\n",
      "B max diff:  0.002127967679820754\n",
      "[0.09396987 0.07313876 0.06280767 0.06229384]\n",
      "iter took:  0.6667683124542236\n",
      "new alpha:  [0.09396987 0.07313876 0.06280767 0.06229384]\n",
      "on iter:  41\n",
      "B max diff:  0.0027320823463805244\n",
      "[0.09365691 0.07291392 0.06249883 0.06212854]\n",
      "iter took:  0.6539804935455322\n",
      "new alpha:  [0.09365691 0.07291392 0.06249883 0.06212854]\n",
      "on iter:  42\n",
      "B max diff:  0.0006718250427192057\n",
      "[0.09279266 0.07264709 0.06185879 0.0619367 ]\n",
      "iter took:  0.8883311748504639\n",
      "new alpha:  [0.09279266 0.07264709 0.06185879 0.0619367 ]\n",
      "on iter:  43\n",
      "B max diff:  0.0009890438190351067\n",
      "[0.0922172  0.07238822 0.06138403 0.0617418 ]\n",
      "iter took:  0.5685863494873047\n",
      "new alpha:  [0.0922172  0.07238822 0.06138403 0.0617418 ]\n",
      "on iter:  44\n",
      "B max diff:  0.0011660243301672753\n",
      "[0.09180261 0.07177496 0.06133594 0.06154797]\n",
      "iter took:  0.6635980606079102\n",
      "new alpha:  [0.09180261 0.07177496 0.06133594 0.06154797]\n",
      "on iter:  45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B max diff:  0.0003498440698841473\n",
      "[0.09091766 0.07130969 0.06156733 0.0610254 ]\n",
      "iter took:  0.7220289707183838\n",
      "new alpha:  [0.09091766 0.07130969 0.06156733 0.0610254 ]\n",
      "on iter:  46\n",
      "B max diff:  0.0002859273387427223\n",
      "[0.09036331 0.07095788 0.06164498 0.06064694]\n",
      "iter took:  0.5696778297424316\n",
      "new alpha:  [0.09036331 0.07095788 0.06164498 0.06064694]\n",
      "on iter:  47\n",
      "B max diff:  0.00023161251695472118\n",
      "[0.0899985  0.07069073 0.06164269 0.06037017]\n",
      "iter took:  2.39213228225708\n",
      "new alpha:  [0.0899985  0.07069073 0.06164269 0.06037017]\n",
      "on iter:  48\n",
      "B max diff:  0.0001863534612935678\n",
      "[0.08974675 0.07048638 0.06160406 0.06016564]\n",
      "iter took:  0.7454507350921631\n",
      "new alpha:  [0.08974675 0.07048638 0.06160406 0.06016564]\n",
      "on iter:  49\n",
      "B max diff:  0.0001487051812200978\n",
      "[0.08956574 0.07032844 0.06155235 0.0600129 ]\n",
      "iter took:  0.8479719161987305\n",
      "new alpha:  [0.08956574 0.07032844 0.06155235 0.0600129 ]\n",
      "on iter:  50\n",
      "B max diff:  0.00012534565755585234\n",
      "[0.08943109 0.07020422 0.06149919 0.05989767]\n",
      "iter took:  0.9819588661193848\n",
      "new alpha:  [0.08943109 0.07020422 0.06149919 0.05989767]\n",
      "on iter:  51\n",
      "B max diff:  0.00014797889161002106\n",
      "[0.08932768 0.07010141 0.06145003 0.05980972]\n",
      "iter took:  0.5892844200134277\n",
      "new alpha:  [0.08932768 0.07010141 0.06145003 0.05980972]\n",
      "on iter:  52\n",
      "B max diff:  0.00027278191772119747\n",
      "[0.08918893 0.06960022 0.06138175 0.05971622]\n",
      "iter took:  0.8935873508453369\n",
      "new alpha:  [0.08918893 0.06960022 0.06138175 0.05971622]\n",
      "on iter:  53\n",
      "B max diff:  0.00013715299644764378\n",
      "[0.08905181 0.06927088 0.06130767 0.05962737]\n",
      "iter took:  1.1558706760406494\n",
      "new alpha:  [0.08905181 0.06927088 0.06130767 0.05962737]\n",
      "on iter:  54\n",
      "B max diff:  0.00010704854001354283\n",
      "[0.08892974 0.06904791 0.06123659 0.05954803]\n",
      "iter took:  0.9947292804718018\n",
      "new alpha:  [0.08892974 0.06904791 0.06123659 0.05954803]\n",
      "on iter:  55\n",
      "B max diff:  9.305697000964314e-05\n",
      "[0.08882552 0.06889173 0.06117023 0.05947961]\n",
      "iter took:  3.3301751613616943\n",
      "new alpha:  [0.08882552 0.06889173 0.06117023 0.05947961]\n",
      "on iter:  56\n",
      "B max diff:  0.00016281954739106436\n",
      "[0.08873738 0.06877811 0.06110795 0.0594217 ]\n",
      "iter took:  1.0429086685180664\n",
      "new alpha:  [0.08873738 0.06877811 0.06110795 0.0594217 ]\n",
      "on iter:  57\n",
      "B max diff:  0.00010948771746530756\n",
      "[0.08866324 0.06869258 0.06105637 0.05937354]\n",
      "iter took:  0.5876410007476807\n",
      "new alpha:  [0.08866324 0.06869258 0.06105637 0.05937354]\n",
      "on iter:  58\n",
      "B max diff:  0.00011087182278411397\n",
      "[0.08860095 0.06862601 0.0610146  0.0593337 ]\n",
      "iter took:  0.8826589584350586\n",
      "new alpha:  [0.08860095 0.06862601 0.0610146  0.0593337 ]\n",
      "on iter:  59\n",
      "B max diff:  0.00012083000626467638\n",
      "[0.08854829 0.06857246 0.06098094 0.05930081]\n",
      "iter took:  0.7722785472869873\n",
      "new alpha:  [0.08854829 0.06857246 0.06098094 0.05930081]\n",
      "on iter:  60\n",
      "B max diff:  0.00013333785973284077\n",
      "[0.08850319 0.06852808 0.06095387 0.0592736 ]\n",
      "iter took:  0.5841364860534668\n",
      "new alpha:  [0.08850319 0.06852808 0.06095387 0.0592736 ]\n",
      "on iter:  61\n",
      "B max diff:  0.0001393408906404922\n",
      "[0.08846346 0.06849034 0.06093208 0.0592509 ]\n",
      "iter took:  0.5273873805999756\n",
      "new alpha:  [0.08846346 0.06849034 0.06093208 0.0592509 ]\n",
      "on iter:  62\n",
      "B max diff:  0.00013737835504476243\n",
      "[0.08842606 0.06845751 0.06091434 0.05923161]\n",
      "iter took:  0.5474724769592285\n",
      "new alpha:  [0.08842606 0.06845751 0.06091434 0.05923161]\n",
      "on iter:  63\n",
      "B max diff:  0.00012779970515726705\n",
      "[0.08838284 0.06842816 0.06089925 0.0592144 ]\n",
      "iter took:  1.4454519748687744\n",
      "new alpha:  [0.08838284 0.06842816 0.06089925 0.0592144 ]\n",
      "on iter:  64\n",
      "B max diff:  0.00032180633076780106\n",
      "[0.08776371 0.06835715 0.06084989 0.0591643 ]\n",
      "iter took:  1.0855023860931396\n",
      "new alpha:  [0.08776371 0.06835715 0.06084989 0.0591643 ]\n",
      "on iter:  65\n",
      "B max diff:  9.646059746557445e-05\n",
      "[0.08740229 0.06827647 0.06079196 0.05910491]\n",
      "iter took:  0.9266960620880127\n",
      "new alpha:  [0.08740229 0.06827647 0.06079196 0.05910491]\n",
      "on iter:  66\n",
      "B max diff:  7.898419475009695e-05\n",
      "[0.08718092 0.06819934 0.06073613 0.05904686]\n",
      "iter took:  2.2105045318603516\n",
      "new alpha:  [0.08718092 0.06819934 0.06073613 0.05904686]\n",
      "on iter:  67\n",
      "B max diff:  7.159503524697985e-05\n",
      "[0.08703787 0.06813034 0.06068669 0.05899409]\n",
      "iter took:  0.9207003116607666\n",
      "new alpha:  [0.08703787 0.06813034 0.06068669 0.05899409]\n",
      "on iter:  68\n",
      "B max diff:  6.897861082360863e-05\n",
      "[0.08694    0.06806999 0.06064468 0.05894729]\n",
      "iter took:  1.2566370964050293\n",
      "new alpha:  [0.08694    0.06806999 0.06064468 0.05894729]\n",
      "on iter:  69\n",
      "B max diff:  6.7059227526683e-05\n",
      "[0.08686893 0.06801704 0.06060964 0.05890563]\n",
      "iter took:  0.8648929595947266\n",
      "new alpha:  [0.08686893 0.06801704 0.06060964 0.05890563]\n",
      "on iter:  70\n",
      "B max diff:  8.93460189181961e-05\n",
      "[0.08681396 0.06796967 0.06058051 0.05886796]\n",
      "iter took:  0.6470699310302734\n",
      "new alpha:  [0.08681396 0.06796967 0.06058051 0.05886796]\n",
      "on iter:  71\n",
      "B max diff:  0.00011669905237744998\n",
      "[0.08676836 0.06792602 0.06055511 0.05883369]\n",
      "iter took:  0.712151050567627\n",
      "new alpha:  [0.08676836 0.06792602 0.06055511 0.05883369]\n",
      "on iter:  72\n",
      "B max diff:  0.00018190838120517002\n",
      "[0.08672628 0.06788359 0.060523   0.05880214]\n",
      "iter took:  1.0166571140289307\n",
      "new alpha:  [0.08672628 0.06788359 0.060523   0.05880214]\n",
      "on iter:  73\n",
      "B max diff:  0.000166258367144992\n",
      "[0.08668381 0.06784038 0.06049195 0.05877303]\n",
      "iter took:  0.8138034343719482\n",
      "new alpha:  [0.08668381 0.06784038 0.06049195 0.05877303]\n",
      "on iter:  74\n",
      "B max diff:  0.00016506544620875077\n",
      "[0.08663535 0.06779354 0.0604667  0.05874569]\n",
      "iter took:  0.8234071731567383\n",
      "new alpha:  [0.08663535 0.06779354 0.0604667  0.05874569]\n",
      "on iter:  75\n",
      "B max diff:  0.0002522550299930479\n",
      "[0.0860102  0.06731847 0.06038638 0.05866502]\n",
      "iter took:  0.7020905017852783\n",
      "new alpha:  [0.0860102  0.06731847 0.06038638 0.05866502]\n",
      "on iter:  76\n",
      "B max diff:  0.00012271791001801976\n",
      "[0.08560213 0.0669769  0.06028912 0.05856504]\n",
      "iter took:  0.6585328578948975\n",
      "new alpha:  [0.08560213 0.0669769  0.06028912 0.05856504]\n",
      "on iter:  77\n",
      "B max diff:  0.00014210010265797244\n",
      "[0.08532237 0.06672404 0.06019266 0.05846116]\n",
      "iter took:  0.7056264877319336\n",
      "new alpha:  [0.08532237 0.06672404 0.06019266 0.05846116]\n",
      "on iter:  78\n",
      "B max diff:  0.00020707409284589397\n",
      "[0.08507751 0.0665028  0.06008214 0.05803819]\n",
      "iter took:  0.8975911140441895\n",
      "new alpha:  [0.08507751 0.0665028  0.06008214 0.05803819]\n",
      "on iter:  79\n",
      "B max diff:  0.00017381175501126906\n",
      "[0.08490806 0.06670278 0.05999833 0.05775823]\n",
      "iter took:  0.9598274230957031\n",
      "new alpha:  [0.08490806 0.06670278 0.05999833 0.05775823]\n",
      "on iter:  80\n",
      "B max diff:  0.0001858151539884021\n",
      "[0.08478499 0.06680754 0.05993342 0.05756899]\n",
      "iter took:  0.847602367401123\n",
      "new alpha:  [0.08478499 0.06680754 0.05993342 0.05756899]\n",
      "on iter:  81\n",
      "B max diff:  0.00019585596866505614\n",
      "[0.08469319 0.06685506 0.05988148 0.0574381 ]\n",
      "iter took:  0.8232302665710449\n",
      "new alpha:  [0.08469319 0.06685506 0.05988148 0.0574381 ]\n",
      "on iter:  82\n",
      "B max diff:  0.00020463115470346644\n",
      "[0.08462183 0.06687049 0.05983542 0.05734523]\n",
      "iter took:  0.656609296798706\n",
      "new alpha:  [0.08462183 0.06687049 0.05983542 0.05734523]\n",
      "on iter:  83\n",
      "B max diff:  0.00023685174661206444\n",
      "[0.08451916 0.06684049 0.05946681 0.05725722]\n",
      "iter took:  1.1939582824707031\n",
      "new alpha:  [0.08451916 0.06684049 0.05946681 0.05725722]\n",
      "on iter:  84\n",
      "B max diff:  0.0002639446836371884\n",
      "[0.08444417 0.06717356 0.05923948 0.05719973]\n",
      "iter took:  0.5166494846343994\n",
      "new alpha:  [0.08444417 0.06717356 0.05923948 0.05719973]\n",
      "on iter:  85\n",
      "B max diff:  0.00027682822750064486\n",
      "[0.08386685 0.06734812 0.05906647 0.05713093]\n",
      "iter took:  0.6131248474121094\n",
      "new alpha:  [0.08386685 0.06734812 0.05906647 0.05713093]\n",
      "on iter:  86\n",
      "B max diff:  0.00026572412757344763\n",
      "[0.08352448 0.06741801 0.05893651 0.05706381]\n",
      "iter took:  0.490095853805542\n",
      "new alpha:  [0.08352448 0.06741801 0.05893651 0.05706381]\n",
      "on iter:  87\n",
      "B max diff:  0.00024559843362173496\n",
      "[0.08336566 0.0682254  0.0585424  0.05703139]\n",
      "iter took:  0.5168230533599854\n",
      "new alpha:  [0.08336566 0.0682254  0.0585424  0.05703139]\n",
      "on iter:  88\n",
      "B max diff:  0.00029622028296597013\n",
      "[0.08328702 0.06872214 0.0583066  0.05701759]\n",
      "iter took:  0.4854722023010254\n",
      "new alpha:  [0.08328702 0.06872214 0.0583066  0.05701759]\n",
      "on iter:  89\n",
      "B max diff:  0.0003593502196965661\n",
      "[0.08271782 0.06898389 0.05817559 0.0569852 ]\n",
      "iter took:  0.5932800769805908\n",
      "new alpha:  [0.08271782 0.06898389 0.05817559 0.0569852 ]\n",
      "on iter:  90\n",
      "B max diff:  0.0004187156196484435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08238914 0.0691121  0.05809137 0.05694855]\n",
      "iter took:  0.5050663948059082\n",
      "new alpha:  [0.08238914 0.0691121  0.05809137 0.05694855]\n",
      "on iter:  91\n",
      "B max diff:  0.0004592893056751643\n",
      "[0.08218771 0.06917009 0.05803558 0.05691335]\n",
      "iter took:  0.453784704208374\n",
      "new alpha:  [0.08218771 0.06917009 0.05803558 0.05691335]\n",
      "on iter:  92\n",
      "B max diff:  0.00046036806824496703\n",
      "[0.08205395 0.0691925  0.05799783 0.05688173]\n",
      "iter took:  0.48672938346862793\n",
      "new alpha:  [0.08205395 0.0691925  0.05799783 0.05688173]\n",
      "on iter:  93\n",
      "B max diff:  0.0004186716260809898\n",
      "[0.08195615 0.06919744 0.05797137 0.05685413]\n",
      "iter took:  0.6296870708465576\n",
      "new alpha:  [0.08195615 0.06919744 0.05797137 0.05685413]\n",
      "on iter:  94\n",
      "B max diff:  0.00035107771482371757\n",
      "[0.08187741 0.0691937  0.05795165 0.05683018]\n",
      "iter took:  0.554924726486206\n",
      "new alpha:  [0.08187741 0.0691937  0.05795165 0.05683018]\n",
      "on iter:  95\n",
      "B max diff:  0.00027650120057396647\n",
      "[0.08180802 0.06918509 0.05793546 0.05680919]\n",
      "iter took:  0.6852519512176514\n",
      "new alpha:  [0.08180802 0.06918509 0.05793546 0.05680919]\n",
      "on iter:  96\n",
      "B max diff:  0.0007855019699638645\n",
      "[0.08075836 0.06952814 0.05788482 0.0567564 ]\n",
      "iter took:  0.8615875244140625\n",
      "new alpha:  [0.08075836 0.06952814 0.05788482 0.0567564 ]\n",
      "on iter:  97\n",
      "B max diff:  0.0002405619436069489\n",
      "[0.08016275 0.06968166 0.05782502 0.05669606]\n",
      "iter took:  0.5645339488983154\n",
      "new alpha:  [0.08016275 0.06968166 0.05782502 0.05669606]\n",
      "on iter:  98\n",
      "B max diff:  0.00012401458341573975\n",
      "[0.07981368 0.06973626 0.05776741 0.05663822]\n",
      "iter took:  0.8612868785858154\n",
      "new alpha:  [0.07981368 0.06973626 0.05776741 0.05663822]\n",
      "on iter:  99\n",
      "B max diff:  0.00011503228546951363\n",
      "[0.07960082 0.06974192 0.05771654 0.05658673]\n",
      "iter took:  0.6493849754333496\n",
      "new alpha:  [0.07960082 0.06974192 0.05771654 0.05658673]\n",
      "on iter:  100\n",
      "B max diff:  0.00025608807213048194\n",
      "[0.07951553 0.07016681 0.05770174 0.05656968]\n",
      "iter took:  0.5346376895904541\n",
      "new alpha:  [0.07951553 0.07016681 0.05770174 0.05656968]\n",
      "on iter:  101\n",
      "B max diff:  0.00030498585992811723\n",
      "[0.07948657 0.07043438 0.05770575 0.05657074]\n",
      "iter took:  0.5455772876739502\n",
      "new alpha:  [0.07948657 0.07043438 0.05770575 0.05657074]\n",
      "on iter:  102\n",
      "B max diff:  0.00032744447762394525\n",
      "[0.0794803  0.07060364 0.05771572 0.05657997]\n",
      "iter took:  0.49918413162231445\n",
      "new alpha:  [0.0794803  0.07060364 0.05771572 0.05657997]\n",
      "on iter:  103\n",
      "B max diff:  0.0005075042472784357\n",
      "[0.07943993 0.07068051 0.05739976 0.05657099]\n",
      "iter took:  0.5590956211090088\n",
      "new alpha:  [0.07943993 0.07068051 0.05739976 0.05657099]\n",
      "on iter:  104\n",
      "B max diff:  0.0008764654865036805\n",
      "[0.07942688 0.07074282 0.05754375 0.05657462]\n",
      "iter took:  0.6126630306243896\n",
      "new alpha:  [0.07942688 0.07074282 0.05754375 0.05657462]\n",
      "on iter:  105\n",
      "B max diff:  0.0014154590633601956\n",
      "[0.07941648 0.07079446 0.05765765 0.05658415]\n",
      "iter took:  0.6274349689483643\n",
      "new alpha:  [0.07941648 0.07079446 0.05765765 0.05658415]\n",
      "on iter:  106\n",
      "B max diff:  0.0014096711872482732\n",
      "[0.07943309 0.07083853 0.05810665 0.05661581]\n",
      "iter took:  0.663050651550293\n",
      "new alpha:  [0.07943309 0.07083853 0.05810665 0.05661581]\n",
      "on iter:  107\n",
      "B max diff:  0.0008007527576841134\n",
      "[0.07946294 0.07090885 0.05842348 0.0566574 ]\n",
      "iter took:  1.968918800354004\n",
      "new alpha:  [0.07946294 0.07090885 0.05842348 0.0566574 ]\n",
      "on iter:  108\n",
      "B max diff:  0.0005446197822137769\n",
      "[0.07950054 0.07097661 0.05864999 0.05670046]\n",
      "iter took:  0.4708669185638428\n",
      "new alpha:  [0.07950054 0.07097661 0.05864999 0.05670046]\n",
      "on iter:  109\n",
      "B max diff:  0.0005424314047717121\n",
      "[0.07953955 0.0710335  0.05881426 0.05674037]\n",
      "iter took:  0.4775545597076416\n",
      "new alpha:  [0.07953955 0.0710335  0.05881426 0.05674037]\n",
      "on iter:  110\n",
      "B max diff:  0.0005352893090992995\n",
      "[0.07910358 0.07107221 0.0589255  0.05706305]\n",
      "iter took:  0.5927941799163818\n",
      "new alpha:  [0.07910358 0.07107221 0.0589255  0.05706305]\n",
      "on iter:  111\n",
      "B max diff:  0.0005217636751725912\n",
      "[0.078843   0.07106807 0.05898402 0.05695482]\n",
      "iter took:  0.5253901481628418\n",
      "new alpha:  [0.078843   0.07106807 0.05898402 0.05695482]\n",
      "on iter:  112\n",
      "B max diff:  0.0005049330622211509\n",
      "[0.07867786 0.07104296 0.05901236 0.05687504]\n",
      "iter took:  0.505126953125\n",
      "new alpha:  [0.07867786 0.07104296 0.05901236 0.05687504]\n",
      "on iter:  113\n",
      "B max diff:  0.00048514881613581553\n",
      "[0.07856572 0.07100925 0.05902376 0.0568151 ]\n",
      "iter took:  0.45676493644714355\n",
      "new alpha:  [0.07856572 0.07100925 0.05902376 0.0568151 ]\n",
      "on iter:  114\n",
      "B max diff:  0.00046266482930755037\n",
      "[0.07852232 0.07138113 0.0590524  0.05679397]\n",
      "iter took:  0.5202372074127197\n",
      "new alpha:  [0.07852232 0.07138113 0.0590524  0.05679397]\n",
      "on iter:  115\n",
      "B max diff:  0.00043445559681206227\n",
      "[0.07850724 0.07161061 0.05908657 0.05679231]\n",
      "iter took:  0.5076732635498047\n",
      "new alpha:  [0.07850724 0.07161061 0.05908657 0.05679231]\n",
      "on iter:  116\n",
      "B max diff:  0.00040033447804153216\n",
      "[0.07850235 0.0717482  0.05911998 0.05679904]\n",
      "iter took:  0.46000194549560547\n",
      "new alpha:  [0.07850235 0.0717482  0.05911998 0.05679904]\n",
      "on iter:  117\n",
      "B max diff:  0.0003773739961455032\n",
      "[0.07855084 0.0722767  0.05917918 0.05683508]\n",
      "iter took:  0.5084211826324463\n",
      "new alpha:  [0.07855084 0.0722767  0.05917918 0.05683508]\n",
      "on iter:  118\n",
      "B max diff:  0.00038922642146913683\n",
      "[0.07861443 0.07261754 0.05924564 0.05688233]\n",
      "iter took:  1.1470472812652588\n",
      "new alpha:  [0.07861443 0.07261754 0.05924564 0.05688233]\n",
      "on iter:  119\n",
      "B max diff:  0.00033678282755615543\n",
      "[0.07867529 0.0728369  0.05930959 0.05693072]\n",
      "iter took:  0.8164806365966797\n",
      "new alpha:  [0.07867529 0.0728369  0.05930959 0.05693072]\n",
      "on iter:  120\n",
      "B max diff:  0.00029200615349988515\n",
      "[0.07872481 0.07298007 0.05936641 0.05697514]\n",
      "iter took:  0.46294617652893066\n",
      "new alpha:  [0.07872481 0.07298007 0.05936641 0.05697514]\n",
      "on iter:  121\n",
      "B max diff:  0.0003034321617613779\n",
      "[0.07880447 0.0735161  0.05944294 0.05703977]\n",
      "iter took:  0.4971046447753906\n",
      "new alpha:  [0.07880447 0.0735161  0.05944294 0.05703977]\n",
      "on iter:  122\n",
      "B max diff:  0.0003613155427531471\n",
      "[0.0789156  0.0739021  0.05954359 0.05743698]\n",
      "iter took:  0.8235137462615967\n",
      "new alpha:  [0.0789156  0.0739021  0.05954359 0.05743698]\n",
      "on iter:  123\n",
      "B max diff:  0.00040951544341475484\n",
      "[0.07856602 0.07416601 0.05963875 0.05804877]\n",
      "iter took:  0.47167015075683594\n",
      "new alpha:  [0.07856602 0.07416601 0.05963875 0.05804877]\n",
      "on iter:  124\n",
      "B max diff:  0.0002585653908507364\n",
      "[0.07841973 0.0743547  0.05972543 0.05846578]\n",
      "iter took:  0.5130386352539062\n",
      "new alpha:  [0.07841973 0.0743547  0.05972543 0.05846578]\n",
      "on iter:  125\n",
      "B max diff:  0.00020050394188749626\n",
      "[0.07841537 0.07452997 0.06015494 0.05877386]\n",
      "iter took:  0.6401236057281494\n",
      "new alpha:  [0.07841537 0.07452997 0.06015494 0.05877386]\n",
      "on iter:  126\n",
      "B max diff:  0.00024082534533599094\n",
      "[0.07843526 0.07465064 0.06013254 0.05898244]\n",
      "iter took:  0.6233787536621094\n",
      "new alpha:  [0.07843526 0.07465064 0.06013254 0.05898244]\n",
      "on iter:  127\n",
      "B max diff:  0.0001841671737518296\n",
      "[0.07850252 0.07516728 0.06016049 0.05915204]\n",
      "iter took:  0.5495991706848145\n",
      "new alpha:  [0.07850252 0.07516728 0.06016049 0.05915204]\n",
      "on iter:  128\n",
      "B max diff:  0.00016417981314810807\n",
      "[0.07858    0.07550852 0.0602095  0.05928781]\n",
      "iter took:  0.5303075313568115\n",
      "new alpha:  [0.07858    0.07550852 0.0602095  0.05928781]\n",
      "on iter:  129\n",
      "B max diff:  0.00020381393105757192\n",
      "[0.07869904 0.07618652 0.06029344 0.05942313]\n",
      "iter took:  0.5401630401611328\n",
      "new alpha:  [0.07869904 0.07618652 0.06029344 0.05942313]\n",
      "on iter:  130\n",
      "B max diff:  0.00022595691036716645\n",
      "[0.07881953 0.07663128 0.06038706 0.05954609]\n",
      "iter took:  0.6959090232849121\n",
      "new alpha:  [0.07881953 0.07663128 0.06038706 0.05954609]\n",
      "on iter:  131\n",
      "B max diff:  0.0005117563466733206\n",
      "[0.07897467 0.07743254 0.06051047 0.05968319]\n",
      "iter took:  0.6294219493865967\n",
      "new alpha:  [0.07897467 0.07743254 0.06051047 0.05968319]\n",
      "on iter:  132\n",
      "B max diff:  0.0006815203114838714\n",
      "[0.07916412 0.0784447  0.06067076 0.05984402]\n",
      "iter took:  0.5226926803588867\n",
      "new alpha:  [0.07916412 0.0784447  0.06067076 0.05984402]\n",
      "on iter:  133\n",
      "B max diff:  0.0011217845940861525\n",
      "[0.07943239 0.07967974 0.06088925 0.06039532]\n",
      "iter took:  0.5531513690948486\n",
      "new alpha:  [0.07943239 0.07967974 0.06088925 0.06039532]\n",
      "on iter:  134\n",
      "B max diff:  0.0015305326378644533\n",
      "[0.07974913 0.08104932 0.06115007 0.06087688]\n",
      "iter took:  0.6067588329315186\n",
      "new alpha:  [0.07974913 0.08104932 0.06115007 0.06087688]\n",
      "on iter:  135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B max diff:  0.0014328774108480283\n",
      "[0.08004113 0.08197839 0.06140829 0.0612793 ]\n",
      "iter took:  0.6015186309814453\n",
      "new alpha:  [0.08004113 0.08197839 0.06140829 0.0612793 ]\n",
      "on iter:  136\n",
      "B max diff:  0.001124964369305221\n",
      "[0.0807892  0.08267417 0.06167231 0.0616377 ]\n",
      "iter took:  0.8550271987915039\n",
      "new alpha:  [0.0807892  0.08267417 0.06167231 0.0616377 ]\n",
      "on iter:  137\n",
      "B max diff:  0.0009847439137363433\n",
      "[0.08136805 0.08374741 0.06194904 0.06197351]\n",
      "iter took:  0.77604079246521\n",
      "new alpha:  [0.08136805 0.08374741 0.06194904 0.06197351]\n",
      "on iter:  138\n",
      "B max diff:  0.0007618419036896397\n",
      "[0.08176427 0.0844498  0.06218754 0.06194058]\n",
      "iter took:  0.6444129943847656\n",
      "new alpha:  [0.08176427 0.0844498  0.06218754 0.06194058]\n",
      "on iter:  139\n",
      "B max diff:  0.0008390557080388689\n",
      "[0.0820824  0.08544097 0.06241667 0.0620024 ]\n",
      "iter took:  1.2197659015655518\n",
      "new alpha:  [0.0820824  0.08544097 0.06241667 0.0620024 ]\n",
      "on iter:  140\n",
      "B max diff:  0.0005973958108271907\n",
      "[0.08232784 0.08607804 0.06261852 0.06210071]\n",
      "iter took:  0.5292956829071045\n",
      "new alpha:  [0.08232784 0.08607804 0.06261852 0.06210071]\n",
      "on iter:  141\n",
      "B max diff:  0.00038482657777982876\n",
      "[0.08250962 0.08649391 0.06278755 0.06220548]\n",
      "iter took:  0.5296590328216553\n",
      "new alpha:  [0.08250962 0.08649391 0.06278755 0.06220548]\n",
      "on iter:  142\n",
      "B max diff:  0.0005426395669173906\n",
      "[0.08224199 0.08729056 0.06328648 0.0623273 ]\n",
      "iter took:  0.5404660701751709\n",
      "new alpha:  [0.08224199 0.08729056 0.06328648 0.0623273 ]\n",
      "on iter:  143\n",
      "B max diff:  0.00045501108102962015\n",
      "[0.0821571  0.08780809 0.06365309 0.0624479 ]\n",
      "iter took:  0.586815595626831\n",
      "new alpha:  [0.0821571  0.08780809 0.06365309 0.0624479 ]\n",
      "on iter:  144\n",
      "B max diff:  0.0003003976606629137\n",
      "[0.08215591 0.08814913 0.06392025 0.06255758]\n",
      "iter took:  0.5603063106536865\n",
      "new alpha:  [0.08215591 0.08814913 0.06392025 0.06255758]\n",
      "on iter:  145\n",
      "B max diff:  0.00017124017232478836\n",
      "[0.08218987 0.08837427 0.06411521 0.06265218]\n",
      "iter took:  0.5255956649780273\n",
      "new alpha:  [0.08218987 0.08837427 0.06411521 0.06265218]\n",
      "on iter:  146\n",
      "B max diff:  0.00023738805404254545\n",
      "[0.08218111 0.08799526 0.06422513 0.06269947]\n",
      "iter took:  0.4814150333404541\n",
      "new alpha:  [0.08218111 0.08799526 0.06422513 0.06269947]\n",
      "on iter:  147\n",
      "B max diff:  9.148018272836763e-05\n",
      "[0.08216072 0.08779872 0.06428692 0.06272231]\n",
      "iter took:  0.5198454856872559\n",
      "new alpha:  [0.08216072 0.08779872 0.06428692 0.06272231]\n",
      "on iter:  148\n",
      "B max diff:  0.0001996646794757105\n",
      "[0.08219397 0.0882368  0.06435539 0.06276168]\n",
      "iter took:  0.5727474689483643\n",
      "new alpha:  [0.08219397 0.0882368  0.06435539 0.06276168]\n",
      "on iter:  149\n",
      "B max diff:  0.00017121559446722778\n",
      "[0.08224405 0.08852302 0.06442076 0.06280218]\n",
      "iter took:  0.5128912925720215\n",
      "new alpha:  [0.08224405 0.08852302 0.06442076 0.06280218]\n",
      "on iter:  150\n",
      "B max diff:  0.00019900878013592513\n",
      "[0.08229314 0.08870039 0.06447767 0.06283405]\n",
      "iter took:  0.6058094501495361\n",
      "new alpha:  [0.08229314 0.08870039 0.06447767 0.06283405]\n",
      "on iter:  151\n",
      "B max diff:  0.0003218101834911169\n",
      "[0.08229651 0.0887683  0.0645009  0.06250248]\n",
      "iter took:  0.6346559524536133\n",
      "new alpha:  [0.08229651 0.0887683  0.0645009  0.06250248]\n",
      "on iter:  152\n",
      "B max diff:  0.0003016189675264817\n",
      "[0.08227508 0.08876822 0.06450335 0.06228644]\n",
      "iter took:  0.6285426616668701\n",
      "new alpha:  [0.08227508 0.08876822 0.06450335 0.06228644]\n",
      "on iter:  153\n",
      "B max diff:  0.0002857504718217765\n",
      "[0.08224048 0.08872209 0.06449268 0.06213847]\n",
      "iter took:  0.8389198780059814\n",
      "new alpha:  [0.08224048 0.08872209 0.06449268 0.06213847]\n",
      "on iter:  154\n",
      "B max diff:  0.0003181264700908024\n",
      "[0.0821633  0.08861341 0.06445148 0.0616863 ]\n",
      "iter took:  0.5359406471252441\n",
      "new alpha:  [0.0821633  0.08861341 0.06445148 0.0616863 ]\n",
      "on iter:  155\n",
      "B max diff:  0.0003979833660514402\n",
      "[0.08206905 0.08849405 0.06439726 0.06137803]\n",
      "iter took:  0.5680725574493408\n",
      "new alpha:  [0.08206905 0.08849405 0.06439726 0.06137803]\n",
      "on iter:  156\n",
      "B max diff:  0.0005510404302398733\n",
      "[0.0819726  0.08838411 0.06434011 0.06116124]\n",
      "iter took:  0.5876030921936035\n",
      "new alpha:  [0.0819726  0.08838411 0.06434011 0.06116124]\n",
      "on iter:  157\n",
      "B max diff:  0.0008465185608907176\n",
      "[0.0818754  0.08828932 0.06428524 0.06100188]\n",
      "iter took:  0.6063847541809082\n",
      "new alpha:  [0.0818754  0.08828932 0.06428524 0.06100188]\n",
      "on iter:  158\n",
      "B max diff:  0.001347124123654488\n",
      "[0.08126909 0.08814741 0.06420194 0.06084964]\n",
      "iter took:  0.5981125831604004\n",
      "new alpha:  [0.08126909 0.08814741 0.06420194 0.06084964]\n",
      "on iter:  159\n",
      "B max diff:  0.001673960666945098\n",
      "[0.08089186 0.08800451 0.06411386 0.06071519]\n",
      "iter took:  0.6169450283050537\n",
      "new alpha:  [0.08089186 0.08800451 0.06411386 0.06071519]\n",
      "on iter:  160\n",
      "B max diff:  0.001259208149699778\n",
      "[0.08064468 0.08787571 0.06403151 0.06060267]\n",
      "iter took:  0.7328691482543945\n",
      "new alpha:  [0.08064468 0.08787571 0.06403151 0.06060267]\n",
      "on iter:  161\n",
      "B max diff:  0.0006018615302761244\n",
      "[0.08047288 0.08775639 0.06395848 0.06051142]\n",
      "iter took:  0.6224167346954346\n",
      "new alpha:  [0.08047288 0.08775639 0.06395848 0.06051142]\n",
      "on iter:  162\n",
      "B max diff:  0.00039287997596789467\n",
      "[0.08038361 0.08769222 0.06426908 0.06046032]\n",
      "iter took:  0.5774550437927246\n",
      "new alpha:  [0.08038361 0.08769222 0.06426908 0.06046032]\n",
      "on iter:  163\n",
      "B max diff:  0.0002926047802449518\n",
      "[0.07991756 0.08819127 0.06450108 0.0607859 ]\n",
      "iter took:  0.5535521507263184\n",
      "new alpha:  [0.07991756 0.08819127 0.06450108 0.0607859 ]\n",
      "on iter:  164\n",
      "B max diff:  0.0004134384029347765\n",
      "[0.07968765 0.08852152 0.06467307 0.06101583]\n",
      "iter took:  0.507706880569458\n",
      "new alpha:  [0.07968765 0.08852152 0.06467307 0.06101583]\n",
      "on iter:  165\n",
      "B max diff:  0.0005269633791571363\n",
      "[0.07957762 0.08873883 0.06480171 0.0611791 ]\n",
      "iter took:  0.5074234008789062\n",
      "new alpha:  [0.07957762 0.08873883 0.06480171 0.0611791 ]\n",
      "on iter:  166\n",
      "B max diff:  0.0005694643557150572\n",
      "[0.079525   0.08888646 0.06489848 0.06129601]\n",
      "iter took:  0.6447992324829102\n",
      "new alpha:  [0.079525   0.08888646 0.06489848 0.06129601]\n",
      "on iter:  167\n",
      "B max diff:  0.000497730323959208\n",
      "[0.07903438 0.08893622 0.06494105 0.0613537 ]\n",
      "iter took:  0.6223089694976807\n",
      "new alpha:  [0.07903438 0.08893622 0.06494105 0.0613537 ]\n",
      "on iter:  168\n",
      "B max diff:  0.00035474433380239994\n",
      "[0.07874545 0.08894066 0.06495571 0.06137909]\n",
      "iter took:  0.5012118816375732\n",
      "new alpha:  [0.07874545 0.08894066 0.06495571 0.06137909]\n",
      "on iter:  169\n",
      "B max diff:  0.00021775479105340216\n",
      "[0.07857174 0.08892701 0.06495667 0.06138695]\n",
      "iter took:  0.5601963996887207\n",
      "new alpha:  [0.07857174 0.08892701 0.06495667 0.06138695]\n",
      "on iter:  170\n",
      "B max diff:  0.000129953795397118\n",
      "[0.07846493 0.08890779 0.06495148 0.06138584]\n",
      "iter took:  0.514211893081665\n",
      "new alpha:  [0.07846493 0.08890779 0.06495148 0.06138584]\n",
      "on iter:  171\n",
      "B max diff:  9.527107769718182e-05\n",
      "[0.0783976  0.08888853 0.06494401 0.06138059]\n",
      "iter took:  0.5006918907165527\n",
      "new alpha:  [0.0783976  0.08888853 0.06494401 0.06138059]\n",
      "on iter:  172\n",
      "B max diff:  0.00010580247653276973\n",
      "[0.07883157 0.08892872 0.06496773 0.06140221]\n",
      "iter took:  0.47822093963623047\n",
      "new alpha:  [0.07883157 0.08892872 0.06496773 0.06140221]\n",
      "on iter:  173\n",
      "B max diff:  0.00013649856956679713\n",
      "[0.07912092 0.08898704 0.0650045  0.06143538]\n",
      "iter took:  0.5902876853942871\n",
      "new alpha:  [0.07912092 0.08898704 0.0650045  0.06143538]\n",
      "on iter:  174\n",
      "B max diff:  0.00010970554622484951\n",
      "[0.0793083  0.08904515 0.06504392 0.06147085]\n",
      "iter took:  0.5656013488769531\n",
      "new alpha:  [0.0793083  0.08904515 0.06504392 0.06147085]\n",
      "on iter:  175\n",
      "B max diff:  0.00012700969784103065\n",
      "[0.07942955 0.08909631 0.06508081 0.06150349]\n",
      "iter took:  0.5670766830444336\n",
      "new alpha:  [0.07942955 0.08909631 0.06508081 0.06150349]\n",
      "on iter:  176\n",
      "B max diff:  0.00013880516418325172\n",
      "[0.079509   0.08913898 0.065113   0.0615308 ]\n",
      "iter took:  0.5337409973144531\n",
      "new alpha:  [0.079509   0.08913898 0.065113   0.0615308 ]\n",
      "on iter:  177\n",
      "B max diff:  0.00011360660567753469\n",
      "[0.07956195 0.08917328 0.06513993 0.06155235]\n",
      "iter took:  0.5080597400665283\n",
      "new alpha:  [0.07956195 0.08917328 0.06513993 0.06155235]\n",
      "on iter:  178\n",
      "B max diff:  8.1593904963746e-05\n",
      "[0.07959704 0.08919987 0.06516179 0.06156881]\n",
      "iter took:  2.5915110111236572\n",
      "new alpha:  [0.07959704 0.08919987 0.06516179 0.06156881]\n",
      "on iter:  179\n",
      "B max diff:  0.00010544150122936628\n",
      "[0.0796196  0.08921967 0.06517908 0.06158113]\n",
      "iter took:  0.514075756072998\n",
      "new alpha:  [0.0796196  0.08921967 0.06517908 0.06158113]\n",
      "on iter:  180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B max diff:  6.477312286980305e-05\n",
      "[0.07963466 0.0892336  0.06519255 0.06159003]\n",
      "iter took:  0.47353124618530273\n",
      "new alpha:  [0.07963466 0.0892336  0.06519255 0.06159003]\n",
      "on iter:  181\n",
      "B max diff:  0.0001441334657975189\n",
      "[0.07964497 0.08924239 0.06520286 0.06159549]\n",
      "iter took:  0.4779949188232422\n",
      "new alpha:  [0.07964497 0.08924239 0.06520286 0.06159549]\n",
      "on iter:  182\n",
      "B max diff:  0.0004917486871774219\n",
      "[0.07965156 0.08924597 0.06521042 0.06159473]\n",
      "iter took:  0.5156247615814209\n",
      "new alpha:  [0.07965156 0.08924597 0.06521042 0.06159473]\n",
      "on iter:  183\n",
      "B max diff:  0.0013752934006374701\n",
      "[0.07965366 0.08924315 0.06521503 0.06157969]\n",
      "iter took:  0.4685788154602051\n",
      "new alpha:  [0.07965366 0.08924315 0.06521503 0.06157969]\n",
      "on iter:  184\n",
      "B max diff:  0.0012756311518977907\n",
      "[0.07964561 0.08923402 0.06521667 0.06155674]\n",
      "iter took:  0.6874792575836182\n",
      "new alpha:  [0.07964561 0.08923402 0.06521667 0.06155674]\n",
      "on iter:  185\n",
      "B max diff:  0.0008686192641278718\n",
      "[0.07962478 0.08921934 0.06521542 0.06153573]\n",
      "iter took:  0.5101556777954102\n",
      "new alpha:  [0.07962478 0.08921934 0.06521542 0.06153573]\n",
      "on iter:  186\n",
      "B max diff:  0.0005685728458378696\n",
      "[0.07959742 0.08920005 0.06521136 0.06151693]\n",
      "iter took:  0.855022668838501\n",
      "new alpha:  [0.07959742 0.08920005 0.06521136 0.06151693]\n",
      "on iter:  187\n",
      "B max diff:  0.0002881429717306017\n",
      "[0.07911489 0.08971398 0.06557929 0.06117013]\n",
      "iter took:  0.6113669872283936\n",
      "new alpha:  [0.07911489 0.08971398 0.06557929 0.06117013]\n",
      "on iter:  188\n",
      "B max diff:  0.0001878570433261409\n",
      "[0.07888958 0.09001246 0.06620334 0.06097604]\n",
      "iter took:  0.6091125011444092\n",
      "new alpha:  [0.07888958 0.09001246 0.06620334 0.06097604]\n",
      "on iter:  189\n",
      "B max diff:  0.0002969124280491854\n",
      "[0.0787966  0.0901873  0.06662112 0.06087184]\n",
      "iter took:  0.7848539352416992\n",
      "new alpha:  [0.0787966  0.0901873  0.06662112 0.06087184]\n",
      "on iter:  190\n",
      "B max diff:  0.0003275773251797823\n",
      "[0.07876833 0.09029003 0.0668993  0.0608197 ]\n",
      "iter took:  0.629497766494751\n",
      "new alpha:  [0.07876833 0.09029003 0.0668993  0.0608197 ]\n",
      "on iter:  191\n",
      "B max diff:  0.00032032331130119854\n",
      "[0.07876915 0.09035016 0.06708454 0.06079717]\n",
      "iter took:  0.4924931526184082\n",
      "new alpha:  [0.07876915 0.09035016 0.06708454 0.06079717]\n",
      "on iter:  192\n",
      "B max diff:  0.0005269584089774587\n",
      "[0.07878081 0.09038425 0.06720687 0.06079106]\n",
      "iter took:  0.5965311527252197\n",
      "new alpha:  [0.07878081 0.09038425 0.06720687 0.06079106]\n",
      "on iter:  193\n",
      "B max diff:  0.0006416999846908364\n",
      "[0.0787945  0.09040114 0.06728559 0.06079338]\n",
      "iter took:  0.4828059673309326\n",
      "new alpha:  [0.0787945  0.09040114 0.06728559 0.06079338]\n",
      "on iter:  194\n",
      "B max diff:  0.00040426451080244905\n",
      "[0.07880667 0.09040553 0.06733533 0.06079884]\n",
      "iter took:  0.41034436225891113\n",
      "new alpha:  [0.07880667 0.09040553 0.06733533 0.06079884]\n",
      "on iter:  195\n",
      "B max diff:  0.00026461110536307557\n",
      "[0.07887323 0.09102487 0.0673977  0.06083832]\n",
      "iter took:  0.5104448795318604\n",
      "new alpha:  [0.07887323 0.09102487 0.0673977  0.06083832]\n",
      "on iter:  196\n",
      "B max diff:  0.00021264226065816766\n",
      "[0.07898802 0.09143222 0.06785082 0.06090991]\n",
      "iter took:  0.5734188556671143\n",
      "new alpha:  [0.07898802 0.09143222 0.06785082 0.06090991]\n",
      "on iter:  197\n",
      "B max diff:  0.00014864711565861465\n",
      "[0.0791074  0.09170023 0.06816971 0.06098993]\n",
      "iter took:  0.47989344596862793\n",
      "new alpha:  [0.0791074  0.09170023 0.06816971 0.06098993]\n",
      "on iter:  198\n",
      "B max diff:  0.00012851589370360512\n",
      "[0.07921213 0.09187826 0.06839143 0.06106596]\n",
      "iter took:  0.5781030654907227\n",
      "new alpha:  [0.07921213 0.09187826 0.06839143 0.06106596]\n",
      "on iter:  199\n",
      "B max diff:  0.00013499426862761385\n",
      "[0.07929634 0.09199472 0.06854581 0.06113191]\n",
      "iter took:  0.6468288898468018\n",
      "new alpha:  [0.07929634 0.09199472 0.06854581 0.06113191]\n",
      "took:  0.6468288898468018\n"
     ]
    }
   ],
   "source": [
    "eta = 10e-5 #threshold for convergence\n",
    "\n",
    "#hyperparamater init.\n",
    "V = len(vectorizer.get_feature_names()) #vocab. cardinality\n",
    "M = int(len(trainDocs)) #number of documents\n",
    "k = 4 #amount of emotions\n",
    "\n",
    "nIter=200 # number of iterations to run until parameter estimation is considered converged\n",
    "\n",
    "#initialize B matrix as random valid distr (most common according to https://profs.sci.univr.it/~bicego/papers/2015_SIMBAD.pdf)\n",
    "B = np.random.rand(k,V)\n",
    "\n",
    "#normalize B\n",
    "for i in range(k):\n",
    "    B[i,:] = B[i]/np.sum(B[i])\n",
    "    \n",
    "alpha = np.ones(k)\n",
    "#variational params (one for each doc)\n",
    "phi = [None]*M\n",
    "gamma = [None]*M\n",
    "\n",
    "#word lists for all docs\n",
    "Wd = [None]*M\n",
    "\n",
    "'''Since scikit gives a matrix of counts of all words, and we want a list of words,\n",
    "we do some quick transformations here. This gives us a representation of the documents \n",
    "as a list of numbers, where each number is the vocabulary index of a word. This way, to access\n",
    "B_ij where i is the ith topic and j is the nth word in the document d, you can simply write B[i][Wd[d][n]]. If you want\n",
    "replace this code a different representation for the words in a document, such as a one-hot vector for each word, you are\n",
    "of course free to do so but make sure to keep track of your indexes'''\n",
    "\n",
    "for d in range(M):\n",
    "    Wmat = vectorizer.transform([trainDocs[d]]).toarray()[0] #get vocabulary matrix for document\n",
    "    WVidxs = np.where(Wmat!=0)[0]\n",
    "    WVcounts = Wmat[WVidxs]\n",
    "    N = np.sum(WVcounts)\n",
    "    W = np.zeros((N)).astype(int)\n",
    "\n",
    "    i = 0\n",
    "    for WVidx, WV in enumerate(WVidxs):\n",
    "        for wordCount in range(WVcounts[WVidx]):\n",
    "            W[i] = WV\n",
    "            i+=1\n",
    "    Wd[d] = W #We save the list of words for the document for analysis later\n",
    "\n",
    "#[print(np.min(Wd[d])) for d in range(M)]\n",
    "start = time.time()\n",
    "\n",
    "#start of parameter estimation loop\n",
    "for j in range(nIter):\n",
    "    print(\"on iter: \", j)\n",
    "    #Variational EM for gamma and phi (E-step from VI section)\n",
    "    start = time.time()\n",
    "    gamma, phi = maxVIParam(phi, gamma, B, alpha, M, k, Wd, eta)\n",
    "    end = time.time()\n",
    "    Bold = np.copy(B)\n",
    "    B = MaxB(B,phi,k,V,M,Wd) #first half of M-step from VI section \n",
    "    # B re-normalized in MaxB\n",
    "    \n",
    "    print(\"B max diff: \", np.amax(abs(B-Bold)))\n",
    "    end = time.time()\n",
    "    \n",
    "    alpha = MaxA(alpha) #second half of M-step from VI section \n",
    "    end = time.time()\n",
    "    print(\"iter took: \", end-start)\n",
    "    print(\"new alpha: \", alpha)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"took: \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VI Parameter Estimation\n",
    "We can now begin with implementing the \"E step\" in the previous section which updates the variational parameters. The pseudo code for this is the following (remember these have to be calculated separately for each document):\n",
    "![VIPseudo](imgs/VIPseudo.png)\n",
    "\n",
    "Since we are working with four topics, k will be set to 4 and N will be the amount of words in the current document. Regarding the \"until convergence\" condition, it is sufficient to check if the largest difference between the previous and new gamma is less than $10^{-5}$. Now, use the pseudo code to fill in the missing code in the \"MaxVIParam\" function defined earlier and remember to use the provided diGamma function. To see that your implementation seems to be working, set nIter to 1 and add some printouts of the difference between updates for gamma, then check that they are converging to something smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beta Matrix Estimation\n",
    "After you have implemented the MaxVIParam function, it's time to update the Beta matrix. Recall that the update function for Beta was:\n",
    "\\begin{equation}\n",
    "\\beta_{ij}\\propto\\sum_{d=1}^M\\sum_{n=1}^{N_d}\\phi^*_{dni}w^j_{dn}\n",
    "\\end{equation}\n",
    "Implement this in the definition for MaxB above. To verify your code, you may set nIter to something low such as 10 and uncomment the \"oldB\" and \"B max diff\" lines in the code. The diff might increase at first but should start decreasing at least before iteration 10. After you have verified this, set nIter to 100 (updates should be negligable by then) and let it run unattended as it might take a couple hours. You can use the code in the following cell to save/load the parameter values you calculated for later so you don't have to re-run everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(alpha, open(\"Data/myAlphaNews100.p\", \"wb\"))\n",
    "pickle.dump(B, open(\"Data/myBetaNews100.p\", \"wb\"))\n",
    "\n",
    "#alpha = pickle.load(open(\"Data/myAlphaNews100.p\", \"rb\"))\n",
    "#B = pickle.load(open(\"Data/myBetaNews100.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verification\n",
    "Let's take a look at what we've done so far. We can get an idea of what our implementation has done up to this point by inspecting the B matrix. As you may remember, B$_{ij}$ holds the probability of a vocabulary word j being representative of a certain topic i. Using the code in the following cell we can see the most representative words for our 4 topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for topic  0 : \n",
      "['team' 'flyer' 'play' 'game' 'hockey' 'ca' 'gm' 'year' 'season' 'player'\n",
      " 'point' 'post' 'goal' 'good' 'leaf' 'city' 'nntp' 'think' 'win' 'time']\n",
      "top words for topic  1 : \n",
      "['car' 'hell' 'say' 'think' 'father' 'know' 'time' 'son' 'spirit' 'thing'\n",
      " 'believe' 'like' 'really' 'eternal' 'way' 'die' 'good' 'use' 'problem'\n",
      " 'heaven']\n",
      "top words for topic  2 : \n",
      "['god' 'people' 'jesus' 'say' 'christian' 'know' 'think' 'religion'\n",
      " 'question' 'believe' 'life' 'world' 'law' 'truth' 'come' 'faith' 'church'\n",
      " 'time' 'christ' 'paul']\n",
      "top words for topic  3 : \n",
      "['space' 'nasa' 'year' 'launch' 'satellite' 'use' 'mission' 'project'\n",
      " 'gov' 'post' 'data' 'access' 'orbit' 'news' 'earth' 'list' 'research'\n",
      " 'probe' 'world' 'program']\n"
     ]
    }
   ],
   "source": [
    "#representation of top words for each topic:\n",
    "nTop = 20\n",
    "for i in range(k):\n",
    "    topVocabs = np.argsort(B[i])[-nTop:][::-1]\n",
    "    topWords = np.array(vectorizer.get_feature_names())[topVocabs]\n",
    "    print(\"top words for topic \",i,\": \")\n",
    "    print(topWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no guarantees regarding the order of the topics (LDA is unsupervised) or what your initial B matrix values were, it is difficult to say exactly what results you should be seeing. Hopefully, you can see the four original topics to some extent in your result. For example, one of my topics had top words like \"christ\", and \"god\", meaning it was most likely the topic for \"Christian Religion\" documents, while another literally had the words \"Hockey\" and \"NHL\" in it. Our vocabulary is as mentioned earlier quite limited, so it may be possible that one of your topics is a bit unclear or close to another. You can also load the $\\alpha$ and $\\beta$ values in the cell below which are pre-calculated for 200 iterations and compare your topic results to those available there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for topic  0 : \n",
      "['space' 'use' 'post' 'nasa' 'nntp' 'program' 'high' '1993' 'distribution'\n",
      " 'year' 'science' 'new' 'question' 'world' 'national' 'development'\n",
      " 'spacecraft' 'launch' 'satellite' 'data']\n",
      "top words for topic  1 : \n",
      "['god' 'people' 'say' 'know' 'christian' 'believe' 'time' 'think' 'good'\n",
      " 'question' 'thing' 'christ' 'way' 'come' 'use' 'make' 'like' 'word'\n",
      " 'bible' 'reason']\n",
      "top words for topic  2 : \n",
      "['team' 'play' 'year' 'player' 'hockey' 'game' 'season' 'nhl' 'contact'\n",
      " 'wing' 'playoff' 'red' '86' 'star' '90' '93' '1992' '92' 'point' 'blue']\n",
      "top words for topic  3 : \n",
      "['think' 'post' 'know' 'car' 'time' 'say' 'like' 'nntp' 'good' 'people'\n",
      " 'use' 'come' 'year' 'way' 'thing' 'really' 'look' 'make' 'work' 'usa']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mangemange\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:306: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.18.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "alphaTest = pickle.load(open(\"Data/CompareAlphaNews200.p\", \"rb\"))\n",
    "BTest = pickle.load(open(\"Data/CompareBetaNews200.p\", \"rb\"))\n",
    "vecTest = pickle.load(open(\"Data/vectorizerNews.p\", \"rb\"), encoding='latin1')\n",
    "nTop = 20\n",
    "for i in range(k):\n",
    "    topVocabs = np.argsort(BTest[i])[-nTop:][::-1]\n",
    "    topWords = np.array(vecTest.get_feature_names())[topVocabs]\n",
    "    print(\"top words for topic \",i,\": \")\n",
    "    print(topWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferring the topic of a test document\n",
    "\n",
    "In this section we will be using our estimated parameter values to infer the topic of some test documents. In order to do this, we will have to calculate the phi and gamma values for each new document we would like to do inference on. This is rather straight forward, and you should be able to reuse your code from the previous sections together with the test documents as a corpus instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eta = 10e-5 #threshold for convergence\n",
    "\n",
    "#we are not re-initializing beta and alpha, we calculated them using the training docs.\n",
    "\n",
    "V = len(vectorizer.get_feature_names()) #vocab. cardinality\n",
    "M = int(len(testDocs)) #number of documents\n",
    "k = 4 #amount of emotions\n",
    "\n",
    "#variational params (one for each doc)\n",
    "phi = [None]*M\n",
    "gamma = [None]*M\n",
    "WdTest = [None]*M\n",
    "\n",
    "'''Same magic from before to get the word matrix correct, replace this if you redid this earlier.'''\n",
    "for d in range(M):\n",
    "    Wmat = vectorizer.transform([testDocs[d]]).toarray()[0] #get vocabulary matrix for document\n",
    "    WVidxs = np.where(Wmat!=0)[0]\n",
    "    WVcounts = Wmat[WVidxs]\n",
    "    N = np.sum(WVcounts)\n",
    "    W = np.zeros((N)).astype(int)\n",
    "\n",
    "    i = 0\n",
    "    for WVidx, WV in enumerate(WVidxs):\n",
    "        for wordCount in range(WVcounts[WVidx]):\n",
    "            W[i] = WV\n",
    "            i+=1\n",
    "    WdTest[d] = W #We save the list of words for the document for analysis later\n",
    "\n",
    "'''Now that you have your variables initialized for the test documents, you should be able to use your previous code for \n",
    "maximizing the VI parameters with those variables instead. Remember, we're just calculating the variational parameters\n",
    "gamma and phi for each test document so there is no iteration between maximizing Beta and maximizing gamma and phi.'''\n",
    "\n",
    "# YOUR CODE to Run the gamma/phi maximization here.\n",
    "\n",
    "gamma, phi = maxVIParam(phi, gamma, B, alpha, M, k, WdTest, eta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now calculated the variational parameters for our test documents, so let us see what information we can infer from them. If you take a look at the pseudo code we used for the MaxVIParam method, you can see that the posterior gamma parameter $\\gamma_i $we are calculating is approximately the prior Dichlet parameter $\\alpha_i$ added to the expected number of words that were generated by that $i^{th}$ topic for a certain document. Looking at the values for the different $\\gamma_i$ over all words for a test document tells us what mixture of topics form such a document. Let us now take a look at the mixtures for some of our test documents by running the code in the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mixture for document  33  is: \n",
      "_______________________\n",
      "topic  0 :  0.8587046557860883\n",
      "topic  1 :  0.0004639789395019427\n",
      "topic  2 :  0.14052308649791462\n",
      "topic  3 :  0.0003082787764950367\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: nlu@Xenon.Stanford.EDU (Nelson Lu)\n",
      "Subject: SHARKS REVIEW Part 3: Defensemen (21-45)\n",
      "Organization: Computer Science Department, Stanford University.\n",
      "Lines: 85\n",
      "\n",
      "#21\tPETER AHOLA\t\tSeason: 2nd\n",
      "Acquired:\t'92-93, trade with Pittsburgh for future considerations\n",
      "Grade:\t\tI (B)\n",
      "\n",
      "It is way too early to tell about Ahola, who was acquired probably because the\n",
      "Penguins figured that they would lose him in the expansion draft.  Ahola had\n",
      "only played 50 games this season (I think it's actually less; the San Jose\n",
      "Mercury News may be in err here), 20 of them with the Sharks.  In the games he\n",
      "has played, he appeared quite solid defensively, although he hasn't been\n",
      "spectacular, and his offense isn't anything to write home about (8 points);\n",
      "it's even possible that the trade may be for future considerations which turn\n",
      "out to be ... Peter Ahola.\n",
      "\n",
      "#24\tDOUG WILSON\t\tSeason: 16th\n",
      "Acquired:\t'91-92, trade with Chicago for RW Kerry Toporowski and\n",
      "\t\t2nd round pick in '92 entry draft\n",
      "Grade:\t\tI (B)\n",
      "\n",
      "I have often been accused of overly down on Wilson; I may have had too high\n",
      "expectations for him, but his legs, knees, et al., are giving out.\n",
      "Nevertheless, when he was playing, he exhibited a strong shooting and\n",
      "playmaking abilities, even if he has lost a step on defense, which,\n",
      "unfortunately, he demonstrated this year as well, as at times he was slow to\n",
      "catch the opponent forwards, and his offensive output was only good enough for\n",
      "2nd place on the team (20 points in 42 games).  But next year, which may be\n",
      "Wilson's last, if he can stay healthy, he can still be a contributor.\n",
      "\n",
      "#29\tDEAN KOLSTAD\t\tSeason:\t2nd\n",
      "Acquired:\t'91-92, from Minnesota in dispersal draft\n",
      "Grade:\t\tI (C-/D+)\n",
      "\n",
      "It's probably somewhat unfair for me to judge Kolstad on just a handful of\n",
      "games (forgetting exact number, but no more than 15), but at age 25 he's\n",
      "quickly running out of time if he wants to make it to the NHL.  In those games,\n",
      "he did not impress anyone; after generating 7 shots in the first period of\n",
      "the first game he played, he scored just 2 points in his tenure up here with\n",
      "the Sharks, and was even less impressive defensively, as he appeared awkward\n",
      "with his movement and was prone to giveaways.  He needs to make a leap in\n",
      "his level of performance to have any chance of making the team.\n",
      "\n",
      "#38\tPAT MACLEOD\t\tSeason: 2nd\n",
      "Acquired:\t'91-92, from Minnesota in dispersal draft\n",
      "Grade:\t\tI (?)\n",
      "\n",
      "MacLeod was on the roster a lot longer than Kolstad, but it appears to my\n",
      "memory that he played less than Kolstad, because the Sharks were reluctant to\n",
      "use him, but were even more reluctant to send him to the minors, figuring that\n",
      "he wouldn't clear waivers; in fact, he has played the past 4-5 weeks with\n",
      "Kansas City, but is still technically there on a rehabilitation assignment,\n",
      "a \"rehab assignment\" that will include him playing in the Turner Cup playoffs.\n",
      "Since he has played so little, I can't even give a tentative grade on him, but\n",
      "he demonstrated last year excellent offensive skills but terrible defensive\n",
      "skills.\n",
      "\n",
      "#41\tTOM PEDERSON\t\tSeason: 1st\n",
      "Acquired:\t'91-92, from Minnesota in dispersal draft\n",
      "Grade:\t\tI (B+)\n",
      "\n",
      "Called up in the middle of the season when the defensive corps was decimated\n",
      "by injuries, Pederson impressed many Sharks fan here on net, including yours\n",
      "truly.  He demonstrated very good offensive skills, scoring 20 points in\n",
      "43 games.  However, his size (5' 9\", 165 lbs.) is of concern, and soon after\n",
      "he began to shine offensive did teams begin to push him around physically,\n",
      "on both sides of the ice, although he had appeared fearless in his approach.\n",
      "But to be successful, he probably needs to bulk up to have a fighting chance\n",
      "on surviving against some of the bigger players in the league.\n",
      "\n",
      "#45\tCLAUDIO SCREMIN\t\tSeason: 1st\n",
      "Acquired:\t'91-92, from Minnesota in dispersal draft\n",
      "Grade:\t\tI (D+/D)\n",
      "\n",
      "He played all of ~5 games in the league this year, but was thoroughly\n",
      "umimpressive, just as he was at the end of last season; again, it may be a\n",
      "small sample, but just as in the case of Kolstad, Scremin, at age 25, is\n",
      "quickly running out of time.  He was not a contributor on either offense or\n",
      "defense in the games he played with the Sharks.  The only notable thing that\n",
      "will go down in Scremin's entry of league stats is probably the fact that he\n",
      "was once traded for now Capitals goaltender Don Beaupre.\n",
      "\n",
      "===============================================================================\n",
      "GO CALGARY FLAMES!  Al MacInnis for Norris!  Gary Roberts for Hart and Smythe!\n",
      "GO EDMONTON OILERS!  Go for playoffs next year!  Stay in Edmonton!\n",
      "===============================================================================\n",
      "Nelson Lu (claudius@leland.stanford.edu)\n",
      "rec.sport.hockey contact for the San Jose Sharks\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  34  is: \n",
      "_______________________\n",
      "topic  0 :  0.8727465250944265\n",
      "topic  1 :  0.12086569914707164\n",
      "topic  2 :  0.0033764883823606084\n",
      "topic  3 :  0.0030112873761412315\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: cdkaupan@eos.ncsu.edu (CARL DAVID KAUPANG)\n",
      "Subject: Stop predicting\n",
      "Originator: cdkaupan@c00544-106ps.eos.ncsu.edu\n",
      "Reply-To: cdkaupan@eos.ncsu.edu (CARL DAVID KAUPANG)\n",
      "Organization: North Carolina State University, Project Eos\n",
      "Lines: 10\n",
      "\n",
      "\n",
      "It is really annoying to see all of these\n",
      "predictions on the Net.  Who really cares\n",
      "who you think will win?  Please stop with\n",
      "the predictions, we all know the Caps are\n",
      "going to win the Cup, so let it go at that.\n",
      "\n",
      "\n",
      "David Kaupang\n",
      "cdkaupan@eos.ncsu.edu\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  35  is: \n",
      "_______________________\n",
      "topic  0 :  0.3104829386750629\n",
      "topic  1 :  0.5099712898055483\n",
      "topic  2 :  0.1781339788319378\n",
      "topic  3 :  0.0014117926874510133\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: gwm@spl1.spl.loral.com (Gary W. Mahan)\n",
      "Subject: Re: It's a rush... (was Re: Too fast)   \n",
      "Organization: Loral Software Productivity Laboratory\n",
      "Lines: 14\n",
      "\n",
      ">>Dont get me wrong, I love to drive in the left lane fast but when I overtake>\n",
      ">>cars who are on the right, I slow down a tad bit. If I were to rely on the j>udgement of the other car, to recognize the speed differential, I would be the stupid one.  \n",
      "\n",
      ">just to satiate my curiosity, why would this make you the stupid one?  It seems\n",
      ">to me, everybody SHOULD be aware enough of what is going on.  You do not need\n",
      "\n",
      "I couldnt agree more.  That is how it SHOULD work. People should also ALWAYS see motorcycles too.\n",
      "\n",
      "I CONSTANTLY scan behind me (I have one of those wink mirrors) and two outside mirrors.  I actually spend just as much time checking my six (cops you know).\n",
      "\n",
      "I still get caught off guard every now and then. \n",
      "\n",
      "\n",
      "Maybe I didnt word it right the first time.  What I was trying to say was that if you plan to blow by somebody at a very HIGH speed differential and you assume you are safe because the guy sees you, you are stupid (of course, it depends on the circumstances).  I have had some VERY scary instances when I assumed this and I dont think all of the fault was the other guy (now if he was going 25 in a 55 thats a whole different story)\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  36  is: \n",
      "_______________________\n",
      "topic  0 :  0.0010818227151264486\n",
      "topic  1 :  0.0012552312079811116\n",
      "topic  2 :  0.15976803125015257\n",
      "topic  3 :  0.8378949148267398\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: rouben@math9.math.umbc.edu (Rouben Rostamian)\n",
      "Subject: Re: Sunrise/ sunset times\n",
      "Organization: University of Maryland, Baltimore County Campus\n",
      "Lines: 60\n",
      "NNTP-Posting-Host: math9.math.umbc.edu\n",
      "\n",
      "In article <1993Apr21.141824.23536@cbis.ece.drexel.edu> jpw@cbis.ece.drexel.edu (Joseph Wetstein) writes:\n",
      ">\n",
      ">Hello. I am looking for a program (or algorithm) that can be used\n",
      ">to compute sunrise and sunset times.\n",
      "\n",
      "Here is a computation I did a long time ago that computes the length\n",
      "of the daylight.  You should be able to convert the information here\n",
      "to sunrise and sunset times.\n",
      "\n",
      "--\n",
      "Rouben Rostamian                          Telephone: 410-455-2458\n",
      "Department of Mathematics and Statistics  e-mail:\n",
      "University of Maryland Baltimore County   bitnet: rostamian@umbc.bitnet\n",
      "Baltimore, MD 21228, USA                  internet: rouben@math.umbc.edu\n",
      "======================================================================\n",
      "Definitions:\n",
      "\n",
      "z = the tilt of the axis of the planet away from the normal to its\n",
      "orbital plane.  In case of the Earth z is about 23.5 degrees, I think.\n",
      "I do not recall the exact value.  In case of Uranus, z is almost\n",
      "90 degrees.\n",
      "\n",
      "u = latitude of the location where the length of the day is measured.\n",
      "Paris is at about 45 degrees.  North pole is at 90.\n",
      "\n",
      "a = angular position of the planet around the sun.  As a goes from\n",
      "0 to 360 degrees, the planet makes a full circle around the sun.\n",
      "The spring equinox occurs at a=0.\n",
      "\n",
      "L = daylight fraction = (duration of daylight)/(duration of a full day).\n",
      "On the equator (u=0) L is always 1/2.  Near the north pole (u=90 degrees)\n",
      "L is sometimes one and sometimes zero, depending on the time of the year.\n",
      "\n",
      "Computation:\n",
      "Define the auxiliary angles p and q by:\n",
      "sin p = sin a sin z\n",
      "cos q = h ( tan u tan p ),       (0 < q < 180 degrees)\n",
      "\n",
      "Conclusion:\n",
      "L = q / 180   (if q is measured in degrees)\n",
      "L = q / pi    (if q is measured in radians)\n",
      "\n",
      "Wait!  But what is h?\n",
      "The cutoff function h is defined as follows:\n",
      "\n",
      "h (s) = s    if  |s| < 1\n",
      "      = 1    if   s > 1\n",
      "      = -1   if   s < 1\n",
      "\n",
      "As an interesting exercise, plot L versus a.   The graph will shows\n",
      "how the length of the daylight varies with the time of the year.\n",
      "Experiment with various choices of latitudes and tilt angles.\n",
      "Compare the behavior of the function at locations above and below\n",
      "the arctic circle.\n",
      "\n",
      "--\n",
      "Rouben Rostamian                          Telephone: 410-455-2458\n",
      "Department of Mathematics and Statistics  e-mail:\n",
      "University of Maryland Baltimore County   bitnet: rostamian@umbc.bitnet\n",
      "Baltimore, MD 21228, USA                  internet: rouben@math.umbc.edu\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  37  is: \n",
      "_______________________\n",
      "topic  0 :  0.005545013274040606\n",
      "topic  1 :  0.49327371588816404\n",
      "topic  2 :  0.004793105752349696\n",
      "topic  3 :  0.49638816508544564\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: tedebear@leland.Stanford.EDU (Theodore Chen)\n",
      "Subject: Re: Ultimate AWD vehicles\n",
      "Organization: DSG, Stanford University, CA 94305, USA\n",
      "Distribution: usa\n",
      "Lines: 7\n",
      "\n",
      "In article <Apr16.215151.28035@engr.washington.edu> eliot@stalfos.engr.washington.edu (eliot) writes:\n",
      ">the price of parts is a different story though...\n",
      "\n",
      "you can say that again.\n",
      "how does $23 for a new thermostat sound?\n",
      "\n",
      "-teddy\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  38  is: \n",
      "_______________________\n",
      "topic  0 :  0.026146758524179827\n",
      "topic  1 :  0.0826348920711165\n",
      "topic  2 :  0.12809862170456326\n",
      "topic  3 :  0.7631197277001405\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: yamauchi@ces.cwru.edu (Brian Yamauchi)\n",
      "Subject: Inflatable Mile-Long Space Billboards (was Re: Vandalizing the sky.)\n",
      "Organization: Case Western Reserve University\n",
      "Lines: 70\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: yuggoth.ces.cwru.edu\n",
      "In-reply-to: enzo@research.canon.oz.au's message of Tue, 20 Apr 1993 22:36:55 GMT\n",
      "\n",
      "In article <C5t05K.DB6@research.canon.oz.au> enzo@research.canon.oz.au (Enzo Liguori) writes:\n",
      ">WHAT'S NEW (in my opinion), Friday, 16 April 1993  Washington, DC\n",
      "\n",
      ">1. SPACE BILLBOARDS! IS THIS ONE THE \"SPINOFFS\" WE WERE PROMISED?\n",
      ">In 1950, science fiction writer Robert Heinlein published \"The\n",
      ">Man Who Sold the Moon,\" which involved a dispute over the sale of\n",
      ">rights to the Moon for use as billboard. NASA has taken the firsteps toward this\n",
      ">hideous vision of the future.  Observers were\n",
      ">startled this spring when a NASA launch vehicle arrived at the\n",
      ">pad with \"SCHWARZENEGGER\" painted in huge block letters on the\n",
      ">side of the booster rockets.  Space Marketing Inc. had arranged\n",
      ">for the ad to promote Arnold's latest movie.\n",
      "\n",
      "Well, if you're going to get upset with this, you might as well direct\n",
      "some of this moral outrage towards Glavcosmos as well.  They pioneered\n",
      "this capitalist application of booster adverts long before NASA.\n",
      "(Sign of the times: a Sony logo on a Soyuz launcher...)\n",
      "\n",
      ">Now, Space Marketing\n",
      ">is working with University of Colorado and Livermore engineers on\n",
      ">a plan to place a mile-long inflatable billboard in low-earth\n",
      ">orbit.\n",
      "\n",
      "This sounds like something Lowell Wood would think of.  Does anyone\n",
      "know if he's involved?\n",
      "\n",
      ">NASA would provide contractual launch services. However,\n",
      ">since NASA bases its charge on seriously flawed cost estimates\n",
      ">(WN 26 Mar 93) the taxpayers would bear most of the expense. This\n",
      ">may look like environmental vandalism, but Mike Lawson, CEO of\n",
      ">Space Marketing, told us yesterday that the real purpose of the\n",
      ">project is to help the environment! The platform will carry ozone\n",
      ">monitors he explained--advertising is just to help defray costs.\n",
      "\n",
      "This may be the purpose for the University of Colorado people.  My\n",
      "guess is that the purpose for the Livermore people is to learn how to\n",
      "build large, inflatable space structures.\n",
      "\n",
      ">..........\n",
      ">What do you think of this revolting and hideous attempt to vandalize\n",
      ">the night sky? It is not even April 1 anymore.\n",
      "\n",
      "If this is true, I think it's a great idea.\n",
      "\n",
      "Learning how to build to build structures in space in an essential\n",
      "step towards space development, and given that Freedom appears to be\n",
      "shrinking towards the vanishing point, I question whether NASA's space\n",
      "station is going to provide much, if any, knowledge in this area.\n",
      "(Especially if a design such as Faget's wingless orbiter is chosen...)\n",
      "If such a project also monitors ozone depletion and demonstrates\n",
      "creative use of (partial) private sector funding in the process -- so\n",
      "much the better.\n",
      "\n",
      ">Is NASA really supporting this junk?\n",
      "\n",
      "And does anyone have any more details other than what was in the WN\n",
      "news blip?  How serious is this project?  Is this just in the \"wild\n",
      "idea\" stage or does it have real funding?\n",
      "\n",
      ">Are protesting groups being organized in the States?\n",
      "\n",
      "Not yet.  Though, if this project goes through, I suppose The Return\n",
      "of Jeremy Rifkin is inevitable...\n",
      "--\n",
      "_______________________________________________________________________________\n",
      "\n",
      "Brian Yamauchi\t\t\tCase Western Reserve University\n",
      "yamauchi@alpha.ces.cwru.edu\tDepartment of Computer Engineering and Science\n",
      "_______________________________________________________________________________\n",
      "\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  39  is: \n",
      "_______________________\n",
      "topic  0 :  0.17060003556324957\n",
      "topic  1 :  0.5175278961212892\n",
      "topic  2 :  0.001547280941763322\n",
      "topic  3 :  0.3103247873736978\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: toml@miles.ca.boeing.com (Tom Locke)\n",
      "Subject: $22600 Subaru SVX -  Good deal?\n",
      "Organization: BoGART Graphics Development\n",
      "Lines: 20\n",
      "\n",
      "Hi netters,\n",
      "\n",
      "My friend is seriously thinking of getting the Subaru SVX. There is\n",
      "a local dealer here in Seattle selling them for $22600, with\n",
      "Touring package, that's $7400 off from MSRP. He thinks it's a \n",
      "very good deal (and I think so too). Since he knows I have access to\n",
      "the net, he would like to get anyone's opinion about this car, especially\n",
      "in the area of reliability and maintenanability.\n",
      "Please send e-mail to me as my friend doesn't have access to the net.\n",
      "\n",
      "My opinion about this car is, you get a lot for $22600:\n",
      "auto everything (tranny, climate control, windows, locks, folddow rear seet),\n",
      "full wheel drive, 2+2, fast (143 top spped), heavy (3580lb);-)\n",
      "\n",
      "Thanks in advacne!\n",
      "-- \n",
      "Tom Locke                         Work: (206) 865-6568\n",
      "Boeing Computer Services        E-mail: toml@voodoo.boeing.com \n",
      "P.O. Box 24346  M/S 7K-20           or: uunet!bcstec!voodoo!toml\n",
      "Seattle, WA  98124-0346\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  40  is: \n",
      "_______________________\n",
      "topic  0 :  0.0005457474732435515\n",
      "topic  1 :  0.22424940333211912\n",
      "topic  2 :  0.6526132541646253\n",
      "topic  3 :  0.12259159503001187\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: creps@lateran.ucs.indiana.edu (Stephen A. Creps)\n",
      "Subject: Re: quality of Catholic liturgy\n",
      "Organization: Indiana University\n",
      "Lines: 72\n",
      "\n",
      "In article <Apr.10.05.30.16.1993.14313@athos.rutgers.edu> jemurray@magnus.acs.ohio-state.edu (John E Murray) writes:\n",
      ">Example.  Last Sunday (Palm Sunday) we went to the local church.  Usually\n",
      ">on Palm Sunday, the congregation participates in reading the Passion, taking\n",
      ">the role of the mob.  The theology behind this seems profound--when we say\n",
      ">\"Crucify him\" we mean it.  We did it, and if He came back today we'd do it\n",
      ">again.  It always gives me chills.  But last week we were \"invited\" to sit\n",
      ">during the Gospel (=Passion) and _listen_.  Besides the Orwellian \"invitation\", \n",
      "\n",
      "   On Palm Sunday at our parish, we were \"invited\" to take the role of\n",
      "Jesus in the Passion.  I declined to participate.  Last year at the\n",
      "liturgy meeting I pointed out how we crucify Christ by our sins, so\n",
      "therefore it is appropriate that we retain the role of the crowd, but\n",
      "to no avail.\n",
      "\n",
      ">musicians, readers, and so on.  New things are introduced in the course of the\n",
      ">liturgy and since no one knows what's happening, the new things have to be\n",
      ">explained, and pretty soon instead of _doing_ a lot of the Mass we're just\n",
      ">sitting there listening (or spacing out, in my case) to how the Mass is about\n",
      ">to be done.  In my mind, I lay the blame on liturgy committees made up of lay\n",
      ">\"experts\", but that may not be just.  I do think that a liturgy committee has a\n",
      ">bias toward doing something rather than nothing--that's just a fact of\n",
      ">bureaucratic life--even though a simpler liturgy may in fact make it easier for\n",
      ">people to be aware of the Lord's presence.\n",
      "\n",
      "   As a member of a liturgy committee, I can tell you that the problem\n",
      "is certain people dominating, who want to try out all kinds of\n",
      "innovations.  The priests don't seem even to _want_ to make any\n",
      "decisions of their own in many cases.  I guess it's easier to \"try\n",
      "something new\" than it is to refuse to allow it.\n",
      "\n",
      "   At our parish on Holy Thursday, instead of the priests washing feet\n",
      "(\"Who wants to get around people's feet,\" according to one of our\n",
      "priests) the congregation was \"invited\" to come up and help wash one\n",
      "another's hands.\n",
      "\n",
      "   The symbolism of this action distressed me, and again I refused to\n",
      "participate.  I thought that if we were to have to come up with\n",
      "rubrics for this liturgical action (i.e. \"Body of Christ\" -- \"Amen\"\n",
      "for receiving Communion), that they could be \"I am not responsible for\n",
      "the blood of this man.\"\n",
      "\n",
      "   Also for part of the Eucharistic Prayer (\"Blessed are You, God of\n",
      "all creation...\") was substituted some text read by a lay couple.  The\n",
      "priest certainly should not have given this part of the Mass over to\n",
      "others, and I was so disturbed that I declined to receive Communion\n",
      "that night (we aren't required to anyway -- I instead offered up\n",
      "prayers for our priests and parish).\n",
      "\n",
      ">So we've been wondering--are we the oddballs, or is the quality of the Mass\n",
      ">going down?  I don't mean that facetiously.  We go to Mass every Thursday or\n",
      ">Friday and are reminded of the power of a very simple liturgy to make us aware \n",
      ">of God's presence.  But as far as the obligatory Sunday Masses...maybe I should \n",
      ">just offer it up :)  Has anyone else noticed declining congregational\n",
      ">participation in Catholic Masses lately?\n",
      "\n",
      "   The quality of the Mass has not changed.  Again, if it were to be\n",
      "celebrated according to the rubrics set down by the Church, it would\n",
      "still be \"liturgically\" beautiful.  The problem comes about from\n",
      "people trying to be \"creative\" who are not.\n",
      "\n",
      "   I think the answer to your question on participation could be that\n",
      "given by Father Peter Stravinskas in answer to the question posed by\n",
      "the title of Thomas Day's _Why Catholics Can't Sing_.  \"They don't\n",
      "want to\" because of all this nonsense.\n",
      "\n",
      "   By the way, for any non-Catholics reading this, the problem does\n",
      "not reflect bad liturgy by the Catholic Church, but by those who are\n",
      "disobedient to the Church in changing it on their own \"authority.\"\n",
      "\n",
      "-\t-\t-\t-\t-\t-\t-\t-\t-\t-\n",
      "Steve Creps, Indiana University\n",
      "creps@lateran.ucs.indiana.edu\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  41  is: \n",
      "_______________________\n",
      "topic  0 :  0.040938415416171534\n",
      "topic  1 :  0.1941129561272471\n",
      "topic  2 :  0.5866074622052702\n",
      "topic  3 :  0.1783411662513112\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: gt7122b@prism.gatech.edu (boundary)\n",
      "Subject: Re: Atheist's views on Christianity (was: Re: \"Accepting Jeesus in your heart...\")\n",
      "Organization: Georgia Institute of Technology\n",
      "Lines: 52\n",
      "\n",
      "In article <Apr.20.03.03.35.1993.3863@geneva.rutgers.edu> jasons@atlastele.com (Jason Smith) writes:\n",
      ">In article <Apr.19.05.13.48.1993.29266@athos.rutgers.edu> kempmp@phoenix.oulu.fi (Petri Pihko) writes:\n",
      "\n",
      ">= This is not true. Science is a collection of models telling us \"how\",\n",
      ">= not why, something happens. I cannot see any good reason why the \"why\"\n",
      ">= questions would be bound only to natural things, assuming that the\n",
      ">= supernatural domain exists. If supernatural beings exist, it is\n",
      ">= as appropriate to ask why they do so as it is to ask why we exist.\n",
      "\n",
      "I beg to disagree with the assertion that science is a collection of models.\n",
      "Scientific models are a game to play, and are only as good as the\n",
      "assumptions and measurements (if any) that go into them.\n",
      "\n",
      "As an example, I remember when nuclear winter was the big hype in\n",
      "atmospheric science.  It wasn't long after Sagan's admonitions that\n",
      "one of our boys was adding another level of reality into his model of\n",
      "the nuclear winter scenario at ERL in Boulder.  He decided to assume\n",
      "that the atmosphere is more like a two-dimensional thing, than a one-\n",
      "dimensional thing.  He also assumed that it rained and that the winds\n",
      "blow in the real atmosphere.  On returning to Georgia Tech, he showed\n",
      "a transparency of atmospheric cooling rates according to the year they\n",
      "were generated by the models.  There was an unmistakable correlation\n",
      "between the age (meaning simplicity of assumptions; i.e., remoteness\n",
      "from reality) of each model and the degree of cooling.  Whereas Sagan's\n",
      "model showed an approximate 40-degree cooling episode, the next model \n",
      "in sophistication showed about half that, and so on until we got to\n",
      "our boy's model, which showed a 1-2 degree drop if the war happened in\n",
      "the winter and less than a 10 degree drop if it happened in the summer.\n",
      "He predicted that when we would include the presence of oceans, chemistry,\n",
      "the biosphere, and other indicators of reality in the models, we would\n",
      "probably see even less cooling.  Thus nuclear winter was reduced to even\n",
      "less than a nuclear autumn, one might say, to a nuclear fizzle.\n",
      "\n",
      "To quote from H.S. Yoder,\n",
      "\n",
      "\tThe postulated models have become accepted as the reality\n",
      "\tinstead of the lattice of assumptions they are.\n",
      "\tAuthoritarianism dominates the field, and a very critical\n",
      "\tanalysis of each argument is to be encouraged.... Skepticism\n",
      "\tof the model approach to earth problems is warranted because\n",
      "\tmany key parameters have not been included.\n",
      "\n",
      "This statement surely applies equally well to cosmogony.  Only when\n",
      "convincing observational evidence substantiates the modeled results\n",
      "may one suggest that the model may describe the reality.  Just thought\n",
      "I'd clear that up before things really got out of hand. \n",
      " \n",
      "-- \n",
      "boundary\n",
      "\n",
      "no teneis que pensar que yo haya venido a traer la paz a la tierra; no he\n",
      "venido a traer la paz, sino la guerra (Mateo 10:34, Vulgata Latina) \n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  42  is: \n",
      "_______________________\n",
      "topic  0 :  0.0981932228975985\n",
      "topic  1 :  0.3479515562466534\n",
      "topic  2 :  0.0010828615224370865\n",
      "topic  3 :  0.552772359333311\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: jaskew@spam.maths.adelaide.edu.au (Joseph Askew)\n",
      "Subject: Re: the call to space (was Re: Clueless Szaboisms )\n",
      "Organization: Statistics, Pure & Applied Mathematics, University of Adelaide\n",
      "Lines: 34\n",
      "\n",
      "In article <1pfiuh$64e@access.digex.com> prb@access.digex.com (Pat) writes:\n",
      "\n",
      ">If the japanese are really going for Nukes, why not go with better\n",
      ">technology then we have.  AS opposed to BWR/PWRs  have they really\n",
      ">considered some of the 3rd generation Inherently safe designs.\n",
      "\n",
      "The Japanese are still on the learning curve as far as nuclear power goes.\n",
      "This means that unlike the Germans (who do great things all by themselves)\n",
      "the Japanese tie up with foreign companies. The major one is Mitsubishi\n",
      "(who else) who have a sharing agreement with GE I think. No chance of a\n",
      "new design.\n",
      "\n",
      ">Sodium has lots of chemical problems but it really solves design\n",
      ">difficulties.  Or the inherently safe types.\n",
      "\n",
      "Sodium has *lots* of chemical problems. Like it eats stainless steel. Very\n",
      "slowly but it gets there in the end. Not what I call a desired property.\n",
      "As for design difficulties, what does sodium do there? It is a bitch and\n",
      "it is only its chemical properties (flwed though they are) that means it\n",
      "gets used. Two loops? That's not a design problem? Isolation from air and\n",
      "water? That doesn't cause design problems? In comparison BWR's a dream rides!\n",
      "\n",
      ">PWR's work real good,  but they need lots of steel,  and they are highly\n",
      ">complex systems.  Simplicity is a virtue.\n",
      "\n",
      "Don't get none of that in a Liquid Sodium Breeder! More steel, more complexity.\n",
      "\n",
      "Joseph Askew\n",
      "\n",
      "-- \n",
      "Joseph Askew, Gauche and Proud  In the autumn stillness, see the Pleiades,\n",
      "jaskew@spam.maths.adelaide.edu  Remote in thorny deserts, fell the grief.\n",
      "Disclaimer? Sue, see if I care  North of our tents, the sky must end somwhere,\n",
      "Actually, I rather like Brenda  Beyond the pale, the River murmurs on.\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  43  is: \n",
      "_______________________\n",
      "topic  0 :  0.5873146690053919\n",
      "topic  1 :  0.008931585789599038\n",
      "topic  2 :  0.397819164195415\n",
      "topic  3 :  0.005934581009594004\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: boora@kits.sfu.ca (The GodFather)\n",
      "Subject: ABC: The Real Thing?\n",
      "Organization: Simon Fraser University, Burnaby, B.C., Canada\n",
      "Lines: 7\n",
      "\n",
      "\n",
      "\tOk, it seems that everyone else in canada was treated to the \n",
      "REAL ABC telecast while only the people on ROGERS TV in Surrey BC \n",
      "were treated to two channels with Don \"I stink as a Commentator\" Whitman\n",
      "doing the play-by-play.\n",
      "\n",
      "\tThe GodFather.\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  44  is: \n",
      "_______________________\n",
      "topic  0 :  0.0011442739339934916\n",
      "topic  1 :  0.23521902444405615\n",
      "topic  2 :  0.599944609760503\n",
      "topic  3 :  0.16369209186144731\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: nichael@bbn.com (Nichael Cramer)\n",
      "Subject: Re: Some questions from a new Christian\n",
      "Reply-To: ncramer@bbn.com\n",
      "Organization: BBN, Interzone Office\n",
      "Lines: 49\n",
      "\n",
      "OFM responds to a query about reference works:\n",
      "\n",
      "   [Aside from a commentary, you might also want to consider an\n",
      "   introduction.  These are books intended for use in undergraduate Bible\n",
      "   courses.  They give historical background, discussion of literary\n",
      "   styles, etc.  And generally they have good bibligraphies for further\n",
      "   reading.  I typically recommend Kee, Froehlich and Young's NT\n",
      "   introduction...\n",
      "\n",
      "Two other Intros to consider:\n",
      "\n",
      "The \"Introduction\" by Ku:mmel is a translation of a strandard NT text.\n",
      "The references are slightly dated and the style is somewhat dense, but\n",
      "the book contains a wealth of information.\n",
      "\n",
      "Perrin and Duling's Intro is also very good.  It's somewhat more\n",
      "modern than Ku:mmel's but not quite so densely packed.  Also the\n",
      "authors tend to go through the books of the NT in the historical order\n",
      "of composition; this gives a very useful perspective on the\n",
      "development of the NT.\n",
      "\n",
      "   ... There are also some good one-volume commentaries.  ... Probably the\n",
      "   best recommendation these days would be Harper's Bible Commentary.\n",
      "\n",
      "A slight dissent: I think the Harper's is \"OK\" but not great.  One\n",
      "particular problem I have is that it tends to be pretty skimpy on\n",
      "bibliographic material.  My feeling is that it is OK for quick\n",
      "look-ups, but not real useful for study in depth (e.g. I keep a copy\n",
      "in my office at work).\n",
      "\n",
      "   ... (I think there may be a couple of books with this title...\n",
      "\n",
      "So far as I know there is the only one book with this exact title\n",
      "(James L Mays, general editor, Harper and ROw, 1988) although I think\n",
      "I recall a (older) series under the name \"Harper Commentaries\".  Also\n",
      "there's a separate Harper's Bible Dictionary (most of my comments on\n",
      "the HC also apply to the HBD.)\n",
      "\n",
      "My favorite one-volume commentary is the \"New Jerome Biblical\n",
      "Commentary\".  The NJBC is rather Catholic in focus and somewhat biased\n",
      "towards the NT.  (The reader can decide for her- or himself whether\n",
      "these are pluses or minuses.)  In any case the scholarship is by and\n",
      "large excellent.\n",
      "\n",
      "NOTE: The NJBC is a completely reworked, updated version of the\n",
      "\"Jerome Biblical Commentary\", copies of which can still be found on\n",
      "sale.\n",
      "\n",
      "Nichael\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  45  is: \n",
      "_______________________\n",
      "topic  0 :  0.9947594415617883\n",
      "topic  1 :  0.002174956758111059\n",
      "topic  2 :  0.0016204350445452804\n",
      "topic  3 :  0.0014451666355551876\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: georgeh@gjhsun (George H)\n",
      "Subject: Re: President Trophy winner missing playoffs ???\n",
      "Organization: Michigan State University\n",
      "Lines: 17\n",
      "NNTP-Posting-Host: gjhsun.cl.msu.edu\n",
      "\n",
      "jstrangi@hora.seas.upenn.edu (Jim Strangio) writes:\n",
      "\n",
      ">In article <13APR93.17376172.0059@VM1.MCGILL.CA> CCDB@MUSICA.MCGILL.CA (CCDB000) writes:\n",
      "\n",
      ">When was the last time a President's Trophy winner fell to last place\n",
      ">the following year?  A long time, I'd bet.\n",
      ">--\n",
      "\n",
      "Well I think it in 1969 the Montreal Canadians finished 1st (although\n",
      "there was no President's cup in those days) and missed the playoffs \n",
      "in 1970. I do recall that the 1970 playoff picture wasn't decided until\n",
      "the final day, when the NY Rangers defeated the RedWings. The tie-breaker was\n",
      "the number of goals for (if I remember correctly), so the Rangers played\n",
      "with an empty net for the entire game. Some Hab loyalists accused \n",
      "the Wings of 'throwing' the game to keep them out, but as I recall, \n",
      "Gordie and Delvecchio had the flu, so coach Sid Abel sat them out,\n",
      "and the Rangers swarmed the RedWings most of the night. \n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  46  is: \n",
      "_______________________\n",
      "topic  0 :  0.0004189078463089681\n",
      "topic  1 :  0.13108040046979857\n",
      "topic  2 :  0.8480757633619873\n",
      "topic  3 :  0.020424928321905258\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: noye@midway.uchicago.edu (vera shanti noyes)\n",
      "Subject: Re: Serbian genocide Work of God?\n",
      "Lines: 89\n",
      "\n",
      "note: i am not the original poster, i am just answering because i\n",
      "think this is important.\n",
      "\n",
      "In article <May.5.02.50.17.1993.28624@athos.rutgers.edu> db7n+@andrew.cmu.edu (D. Andrew Byler) writes:\n",
      ">revdak@netcom.com writes:\n",
      "[evil result of human sinfulness, rather than the will of God]\n",
      ">In a certain sense yes.  But in the sense that God allows evil to\n",
      ">happen, when obviously (He being God) He could have not had it happen,\n",
      ">does in a certain sense mean that He wills it to happen.  God does not\n",
      ">condone evil, but instead uses it for good, as you say, however, what\n",
      ">God desires, must be seperated from what actually happens.  For example,\n",
      ">\"God desires that all should be saved\" (1 Timothy 2.4), however, it is\n",
      ">quite obvious that nowhere near all are saved.  Was God's will thwarted?\n",
      "> No, because His will cannot be escaped, for even when it appears that\n",
      ">it is your will doing something, it is actually the will of God which by\n",
      ">His grace has disposed us to do as He wishes.  So we come to the age old\n",
      ">question, why does evil occur?  To which we must answer that God allows\n",
      ">evil to occur, though He does not condone it, so that His ultimate plan\n",
      ">may be brought to sucess.  Personally, I suggest reading the parts of\n",
      ">the Summa Theologica of St. Thomas that deal with the knowledge of God\n",
      ">to get a good grasp on this whole idea.\n",
      "\n",
      "whoo.  i'm going to have to be very careful with my language here.  i\n",
      "think God is voluntarily giving up his omniscience in this world so\n",
      "that we can decide on our own where we go -- free will.  in this sense\n",
      "God allows evil to occur, and in this sense can be \"held responsible\"\n",
      "as my chaplain says.  however, his will is, of course, that all be\n",
      "saved.  he's not going to save us \"by himself\" -- we have to take a\n",
      "step in his direction before he will save us.  read that last sentence\n",
      "carefully -- i'm not saying we save ourselves.  i'm saying we have to\n",
      "ACCEPT our salvation.  i do not believe in predestination -- it would\n",
      "appear from what you say further down that you do.  \n",
      "\n",
      "[stuff deleted]\n",
      ">I am not saying that anyone deserves punishment more than someone else.\n",
      ">I am simply pointing out that God could be using the Serbians and\n",
      ">Croatians as instruments of His punishment, as he did with the\n",
      ">Israelites against the Cannanites.\n",
      "\n",
      "ok -- i have trouble with that, but i guess that's one of those things\n",
      "that can't be resolved by argument.  i accept your interpretation.\n",
      "\n",
      "[more deleted]\n",
      ">>The issue is not questioning why God has made the world in the way God\n",
      ">>so chooses, it is whether _I_ am discerning the world in the way God\n",
      ">>intends it.  The debate is about whether we should not oppose the Serbians\n",
      ">>in their \"ethnic cleansing\" because they might be \"doing the will of God.\"\n",
      ">\n",
      ">And I said Christians should not be participants in such wars and\n",
      ">slaughters. That does not mitigate the fact that God allows this evil to\n",
      ">continue, for He is patient and willing that none should perish, so He\n",
      ">waits for those whom He has foreknown to turn to Him from their evil.\n",
      "\t\t\t     ^^^^^^^^^\n",
      "this is what indicates to me that you may believe in predestination.\n",
      "am i correct?  i do not believe in predestination -- i believe we all\n",
      "choose whether or not we will accept God's gift of salvation to us.\n",
      "again, fundamental difference which can't really be resolved.\n",
      "\n",
      "[yet more deleted]\n",
      ">I am not saying that the evil befalling the Bosnians is justified by\n",
      ">their guilt.  I am saying that it is possible that God is punishing them\n",
      ">in this way. In no way is this evil justified, bu that does not mean\n",
      ">that God cannot use evil to further His purposes.  I am not accusing the\n",
      ">Bosnians, though they may very well be guilty of great sins, but that is\n",
      ">up to God to judge.  We are all defendants when the time comes for our\n",
      ">judgement by God.  Let us all sincerely hope and pray that we will have\n",
      ">Jesus Christ as our advocate at that judgement.\n",
      "\n",
      "yes, it is up to God to judge.  but he will only mete out that\n",
      "punishment at the last judgement.  as for now, evil can be done by\n",
      "human beings that is NOT God's will -- and the best we can do is see\n",
      "taht some good comes out of it somehow.  the thing that most worries\n",
      "me about the \"it is the will of God\" argument is that this will\n",
      "convince people that we should not STOP the rape and killing when i\n",
      "think that it is most christ-like to do just that.  if jesus stopped\n",
      "the stoning of an adulterous woman (perhaps this is not a good\n",
      "parallel, but i'm going to go with it anyway), why should we not stop\n",
      "the murder and violation of people who may (or may not) be more\n",
      "innocent?\n",
      "\n",
      ">Andy Byler\n",
      "\n",
      "vera\n",
      "*******************************************************************************\n",
      "I am your CLOCK!     |  I bind unto myself today    | Vera Noyes\n",
      "I am your religion!  |  the strong name of the\t    | noye@midway.uchicago.edu\n",
      "I own you!\t     |  Trinity....\t\t    | no disclaimer -- what\n",
      "\t- Lard\t     |\t- St. Patrick's Breastplate | is there to disclaim?\n",
      "*******************************************************************************\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  47  is: \n",
      "_______________________\n",
      "topic  0 :  0.0015161898871820874\n",
      "topic  1 :  0.001759198776946942\n",
      "topic  2 :  0.8716174965514243\n",
      "topic  3 :  0.12510711478444672\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: sdittman@liberty.uc.wlu.edu (Scott Dittman)\n",
      "Subject: Re: Some questions from a new Christian\n",
      "Organization: Washington & Lee University\n",
      "Lines: 21\n",
      "\n",
      "Steven R Hoskins (18669@bach.udel.edu) wrote:\n",
      ": Hi,\n",
      "\n",
      ": I am new to this newsgroup, and also fairly new to christianity.\n",
      ": ... I realize I am very ignorant about much of the Bible and\n",
      ": quite possibly about what Christians should hold as true. This I am trying\n",
      ": to rectify (by reading the Bible of course), but it would be helpful\n",
      ": to also read a good interpretation/commentary on the Bible or other\n",
      ": relevant aspects of the Christian faith. One of my questions I would\n",
      ": like to ask is - Can anyone recommend a good reading list of theological\n",
      ": works intended for a lay person?\n",
      "\n",
      "I'd recommend McDowell's \"Evidence that Demands a Verdict\" books (3 I\n",
      "think) and  Manfred Brauch's \"Hard Sayings of Paul\".  He also may have\n",
      "done \"Hard Sayings of Jesus\".  My focus would be for a new Christian to\n",
      "struggle with his faith and be encouraged by the historical evidence,\n",
      "especially one who comes from a background which emphasizes knowable faith.\n",
      "-- \n",
      "Scott Dittman                    email: sdittman@wlu.edu\n",
      "University Registrar             talk: (703)463-8455   fax: (703)463-8024\n",
      "Washington and Lee University    snail mail:  Lexington Virginia 24450\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  48  is: \n",
      "_______________________\n",
      "topic  0 :  0.5837933958728924\n",
      "topic  1 :  0.4114566636555928\n",
      "topic  2 :  0.0025107535724534216\n",
      "topic  3 :  0.0022391868990613837\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: brandt@cs.unc.edu (Andrew Brandt)\n",
      "Subject: 4Runner and Pathfinder recent changes.\n",
      "Organization: The University of North Carolina at Chapel Hill\n",
      "Lines: 9\n",
      "NNTP-Posting-Host: axon.cs.unc.edu\n",
      "Keywords: 4runner, pathfinder, change, update\n",
      "\n",
      "I am interested in finding out how the 4Runner and Pathfinder have\n",
      "been updated in the past few years.  Like new engine, suspension and\n",
      "the like.  I noticed that the 1993 and 1992 4Runners are identical,\n",
      "for example, and was looking into buying a used one.\n",
      "\n",
      "Any info would be appreciated, esp. models/years to check out or\n",
      "avoid.\n",
      "\n",
      "Thx, Andy (brandt@cs.unc.edu)\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  49  is: \n",
      "_______________________\n",
      "topic  0 :  0.7591604121164676\n",
      "topic  1 :  0.2364138630286731\n",
      "topic  2 :  0.0023393773045092213\n",
      "topic  3 :  0.002086347550350225\n",
      "_______________________\n",
      "Which has the following text:\n",
      " \n",
      "From: icop@csa.bu.edu (Antonio Pera)\n",
      "Subject: ABC coverage\n",
      "Distribution: usa\n",
      "Organization: Computer Science Department, Boston University, Boston, MA, USA\n",
      "Lines: 9\n",
      "Originator: icop@csa\n",
      "\n",
      "\n",
      "\tI loved the ABC coverage. The production was excellent. The appearance\n",
      "was excellent. It had a sleek modern look. This was the first time I heard\n",
      "Thorne & Clement & I thought they were great. My only request is to leave\n",
      "Al Micheals out of this. He annoys me. \n",
      "\tI'm hoping this leads to a regular-season contract. My guess would\n",
      "be is that it will be roughly a weekly game from Feb.-April and then the \n",
      "playoffs. I envy you Canadians with your TSN & CBC. Maybe I'll get a dish\n",
      "to pick up Canadian TV. How much are those things, BTW?\n",
      "\n",
      "__________________________________________\n",
      "__________________________________________\n",
      "Estimated mixture for document  50  is: \n",
      "_______________________\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5612fe32101a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Estimated mixture for document \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" is: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_______________________\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"topic \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\": \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_______________________\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#take a look at some example test documents (14-24 has a nice mix of topics, with a couple difficult ones)\n",
    "dStart = 33\n",
    "dEnd = 340\n",
    "for d in range(dStart,dEnd):\n",
    "    print(\"Estimated mixture for document \", d,\" is: \")\n",
    "    print(\"_______________________\")\n",
    "    for i in range(len(gamma[d])):\n",
    "        print(\"topic \", i,\": \", gamma[d][i]/np.sum(gamma[d]))\n",
    "    print(\"_______________________\")\n",
    "    print(\"Which has the following text:\")\n",
    "    print(\" \")\n",
    "    print(origTestDocs[d])\n",
    "    #print(trainDocs[d])\n",
    "    print(\"__________________________________________\")\n",
    "    print(\"__________________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisit the cell that presented the top words for your topics. Do the presented mixtures above make sense if you look at the document content? Recall which original categories (\"Religion\", \"Cars\", \"Hockey\", \"Space\") you (to your best ability) assigned to which numbers. Do the texts seem to be discussing those topics?\n",
    "\n",
    "<span style=\"color:blue\">If you're re-doing this test with the MoodyLyrics dataset from the bonus section, you may be noticing some weird results. LDA can experience some issues in this setting, as for example many words that would be present in a happy song could also be present in a sad song ('love', 'hold', 'forever') but in different order or with certain \"negating\" words between them. It is possible to alleviate this problem by using a vocabulary of n-grams, however this increases the total size of the vocabulary (and therefore the run time as well) substantially. </span>\n",
    "\n",
    "It is also possible to gain some more insight by examining the $\\phi$ values for the documents. Recall that the $\\phi$ values for the document approximate $p(z_n | \\mathbf{w})$, showing how the topics are mixed for the words in the document. The cell below provides a method for printing the phi values for each word in a document. Apart from just examining how the topics are mixed for specific words, take a look at the topic mixtures for the same word that appears in several different documents. As stated in the theory section, in LDA the distribution of topic mixtures that are assigned to each word is sampled differently for each document. This means that hopefully it should be apparent from your results how the topic mixture for the same word can be differ in different test documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eternal\t: [0.00000000e+00 1.00000000e+00 1.04154322e-81 0.00000000e+00]\n",
      "yes\t: [6.31406106e-009 3.10193409e-001 6.89806585e-001 1.25388445e-213]\n",
      "16\t: [2.60426703e-08 1.01691476e-01 8.98308497e-01 2.21982931e-10]\n",
      "come\t: [9.06819888e-09 8.78717863e-02 9.12128205e-01 6.65115606e-11]\n",
      "problem\t: [8.65180835e-09 5.25422855e-01 4.74577136e-01 7.70178711e-27]\n",
      "important\t: [1.21240881e-08 2.78114751e-01 7.21885237e-01 7.56278445e-54]\n",
      "talk\t: [5.44592467e-21 1.81470932e-01 8.18529068e-01 2.70324263e-62]\n",
      "lead\t: [3.86754724e-08 8.89783644e-02 9.11021597e-01 3.60050543e-10]\n",
      "answer\t: [2.07796893e-165 2.49533791e-048 1.00000000e+000 1.78873467e-010]\n",
      "accord\t: [6.22080568e-25 1.84501932e-01 8.15498068e-01 1.61822877e-10]\n",
      "christian\t: [0.         0.02397333 0.97602667 0.        ]\n",
      "rick\t: [1.01341432e-07 0.00000000e+00 9.99999899e-01 0.00000000e+00]\n",
      "uk\t: [0.00000000e+000 1.00000000e+000 1.08952544e-020 4.14959549e-151]\n",
      "help\t: [6.68498733e-09 1.63908268e-01 8.36091725e-01 1.84629374e-10]\n",
      "life\t: [3.84428786e-93 5.67239762e-02 9.43276024e-01 7.67662766e-11]\n",
      "faith\t: [0.         0.03125251 0.96874749 0.        ]\n",
      "statement\t: [1.20878496e-223 2.64805569e-001 7.35194431e-001 1.88753423e-077]\n",
      "reply\t: [3.14258040e-08 1.46907347e-01 8.53092621e-01 6.44471434e-10]\n",
      "rutgers\t: [0.00000000e+000 1.63026578e-068 1.00000000e+000 1.24770194e-275]\n",
      "mot\t: [8.63926074e-09 2.29745744e-53 9.99999991e-01 5.20417334e-63]\n",
      "02\t: [2.91446294e-08 1.30538242e-01 8.69461727e-01 1.69952354e-09]\n",
      "pt\t: [3.39147470e-08 0.00000000e+00 9.99999966e-01 0.00000000e+00]\n",
      "believe\t: [1.68817252e-009 2.21058749e-001 7.78941249e-001 6.69870297e-231]\n",
      "guess\t: [6.84307968e-08 9.99960822e-01 3.91092418e-05 5.44592519e-10]\n",
      "live\t: [1.09551056e-80 1.11868725e-01 8.88131275e-01 3.11554646e-11]\n",
      "14\t: [2.21084664e-08 1.83164425e-02 9.81683535e-01 2.69719282e-10]\n",
      "group\t: [1.91356531e-93 2.69064040e-02 9.73093596e-01 3.75047901e-10]\n",
      "james\t: [3.06563067e-22 1.00000000e+00 9.73301447e-16 6.56769625e-38]\n",
      "point\t: [5.70657938e-08 3.10619261e-01 6.89380681e-01 2.61037229e-10]\n",
      "right\t: [2.42762506e-08 1.84612615e-05 9.99981514e-01 1.20164062e-10]\n",
      "close\t: [1.83586399e-08 2.98484548e-52 9.99999981e-01 4.96815332e-10]\n",
      "save\t: [1.89151782e-008 3.44628052e-001 6.55371929e-001 3.59469364e-223]\n",
      "possible\t: [2.59022344e-103 8.54752413e-002 9.14524758e-001 9.48182073e-010]\n",
      "god\t: [6.60225166e-237 5.53066077e-002 9.44693392e-001 0.00000000e+000]\n",
      "offer\t: [5.35225203e-09 1.29642832e-01 8.70357162e-01 8.33416209e-10]\n",
      "apr\t: [2.15383794e-09 1.30632122e-50 9.99999998e-01 1.05571215e-10]\n",
      "situation\t: [1.29769852e-008 2.83835096e-001 7.16164891e-001 1.25626496e-288]\n",
      "remember\t: [5.96518847e-08 9.99999940e-01 5.43518352e-28 1.72753268e-44]\n",
      "51\t: [3.54694978e-08 6.97079674e-55 9.99999965e-01 1.20058576e-69]\n",
      "26\t: [1.38413986e-07 3.23388981e-01 6.76610880e-01 9.61727891e-10]\n",
      "work\t: [4.68964834e-09 4.14306649e-01 5.85693346e-01 2.75747775e-10]\n",
      "21\t: [2.85071429e-08 7.42907003e-29 9.99999971e-01 6.25414639e-10]\n",
      "bible\t: [0.00000000e+000 5.84629480e-002 9.41537052e-001 3.11189041e-291]\n",
      "teaching\t: [0.00000000e+00 5.16368237e-92 1.00000000e+00 0.00000000e+00]\n",
      "especially\t: [4.47314595e-136 2.91083663e-001 7.08916337e-001 5.91827980e-010]\n",
      "people\t: [8.03498115e-10 8.08973135e-02 9.19102686e-01 1.51669999e-11]\n",
      "say\t: [5.34635976e-09 1.85532225e-01 8.14467770e-01 9.46100727e-11]\n",
      "church\t: [0.00000000e+000 2.61169909e-002 9.73883009e-001 4.61631539e-213]\n",
      "opinion\t: [1.02097559e-08 5.48883812e-02 9.45111609e-01 8.96371813e-11]\n",
      "think\t: [1.12400924e-08 2.14666295e-01 7.85333694e-01 6.21944840e-11]\n",
      "1993\t: [1.04134068e-08 9.53654653e-35 9.99999989e-01 3.69445541e-10]\n",
      "need\t: [1.03691216e-08 9.60383499e-02 9.03961640e-01 1.37794007e-10]\n",
      "like\t: [8.32569006e-09 2.65724134e-01 7.34275857e-01 2.06804058e-10]\n",
      "passage\t: [0.00000000e+00 5.41300730e-64 1.00000000e+00 1.90304077e-10]\n",
      "jesus\t: [0.00000000e+00 5.15499641e-13 1.00000000e+00 0.00000000e+00]\n",
      "know\t: [6.62600781e-09 1.70221423e-01 8.29778570e-01 7.59080479e-11]\n",
      "roman\t: [0.00000000e+000 3.37906191e-111 1.00000000e+000 0.00000000e+000]\n",
      "person\t: [4.06165819e-70 1.92683692e-01 8.07316308e-01 3.04406153e-10]\n",
      "mind\t: [5.22364337e-09 4.70044574e-02 9.52995537e-01 1.40041885e-10]\n",
      "body\t: [2.73387106e-09 1.49265197e-01 8.50734800e-01 5.73445163e-11]\n",
      "ac\t: [8.33253557e-31 7.08299904e-01 2.91700096e-01 1.02792844e-76]\n",
      "sense\t: [2.55852845e-145 9.93098485e-002 9.00690151e-001 5.02718174e-010]\n",
      "christ\t: [0.         0.02312508 0.97687492 0.        ]\n",
      "make\t: [1.81819788e-08 1.77412419e-01 8.22587562e-01 2.14434039e-10]\n",
      "bit\t: [4.35121554e-08 9.99999956e-01 5.08615118e-28 2.40718707e-27]\n",
      "________________________________\n",
      "colorado\t: [5.98028011e-161 9.99999973e-001 2.49724676e-008 2.14713560e-009]\n",
      "30\t: [9.99999998e-01 2.37247813e-54 1.04858114e-09 7.90717273e-10]\n",
      "31\t: [9.60190565e-01 3.98094330e-02 2.04151491e-09 1.03529834e-10]\n",
      "team\t: [1.00000000e+000 2.63473515e-240 7.81665350e-276 3.18040791e-011]\n",
      "air\t: [9.99999999e-001 1.32999876e-309 6.39499773e-010 1.16604599e-010]\n",
      "23\t: [9.92290322e-01 7.70967625e-03 1.56435186e-09 2.44175851e-10]\n",
      "new\t: [9.68593167e-01 3.14068244e-02 7.91899254e-09 1.01083735e-09]\n",
      "quebec\t: [1. 0. 0. 0.]\n",
      "18\t: [9.99999998e-01 8.59317375e-36 1.18787762e-09 3.57632699e-10]\n",
      "nntp\t: [9.65778009e-01 3.42219910e-02 1.26536972e-32 3.95482757e-10]\n",
      "pick\t: [9.81257850e-01 1.87421500e-02 3.09964072e-10 0.00000000e+00]\n",
      "41\t: [9.55622690e-01 4.43773083e-02 2.86885394e-22 1.40777394e-09]\n",
      "27\t: [9.71920776e-01 2.80792224e-02 1.42732801e-09 4.41170466e-10]\n",
      "pittsburgh\t: [0.97985839 0.02014161 0.         0.        ]\n",
      "buffalo\t: [1. 0. 0. 0.]\n",
      "good\t: [9.21200201e-01 7.87997960e-02 2.27794186e-09 2.91240939e-10]\n",
      "calgary\t: [9.72208567e-001 2.77914326e-002 1.05442444e-105 7.92812864e-172]\n",
      "11\t: [9.99171989e-01 8.28008695e-04 1.90702764e-09 3.02163619e-10]\n",
      "angeles\t: [9.99999999e-001 6.40115631e-122 2.08205410e-159 5.45237224e-010]\n",
      "michael\t: [9.82733447e-01 1.72665511e-02 1.96238437e-09 1.33524600e-10]\n",
      "year\t: [9.89627364e-01 1.03726347e-02 8.15127577e-10 6.00255111e-10]\n",
      "people\t: [5.83008803e-01 4.16991143e-01 5.40247313e-08 2.56257097e-10]\n",
      "canada\t: [9.99999998e-01 0.00000000e+00 1.31898456e-09 4.99224636e-10]\n",
      "state\t: [8.97015192e-001 1.02984808e-001 9.89444242e-165 6.64289340e-010]\n",
      "2nd\t: [9.99999997e-01 1.63647739e-42 1.69558771e-09 1.14838983e-09]\n",
      "begin\t: [2.82423202e-283 9.99999970e-001 2.35009239e-008 6.05675327e-009]\n",
      "38\t: [9.99999999e-001 8.97647833e-125 7.26862592e-010 1.05295586e-101]\n",
      "21\t: [9.99999997e-01 1.85132744e-29 2.84174186e-09 5.10858138e-10]\n",
      "40\t: [9.62189691e-01 3.78103086e-02 7.52134488e-23 2.72677415e-10]\n",
      "26\t: [9.83673292e-01 1.63267071e-02 3.89536547e-10 1.59150894e-10]\n",
      "louis\t: [9.29431873e-01 7.05681245e-02 2.89119665e-09 0.00000000e+00]\n",
      "winnipeg\t: [1.00000000e+000 1.40457532e-280 9.94750501e-162 6.57591655e-108]\n",
      "include\t: [9.71741599e-01 2.82583985e-02 4.96773153e-10 1.60863032e-09]\n",
      "final\t: [9.77611983e-01 2.23880172e-02 0.00000000e+00 2.76070088e-10]\n",
      "washington\t: [9.99999999e-001 1.73769043e-084 2.19428196e-247 7.06413395e-010]\n",
      "los\t: [9.99999999e-001 9.98374205e-049 6.12719789e-197 5.58089872e-010]\n",
      "thanks\t: [6.78693955e-01 3.21306042e-01 2.25572332e-09 1.00409030e-09]\n",
      "15\t: [9.92385211e-01 7.61478658e-03 2.35038754e-09 5.46466459e-10]\n",
      "14\t: [9.94148911e-01 5.85108534e-03 3.57604242e-09 2.82416968e-10]\n",
      "500\t: [9.35130104e-01 6.48698963e-02 8.73151457e-19 9.17917385e-11]\n",
      "mike\t: [1.00000000e+00 2.15125804e-50 4.14971503e-10 8.17245047e-11]\n",
      "montreal\t: [1. 0. 0. 0.]\n",
      "summary\t: [1.00000000e+000 1.24081194e-106 4.03739057e-010 8.68160960e-011]\n",
      "student\t: [0.00000000e+00 9.99999990e-01 9.99730121e-56 1.00133157e-08]\n",
      "chicago\t: [9.99999999e-001 4.33510551e-145 1.06583525e-009 0.00000000e+000]\n",
      "post\t: [9.56237967e-01 4.37620314e-02 1.07371387e-09 5.24199755e-10]\n",
      "st\t: [8.81742117e-001 1.18257883e-001 2.97884907e-256 5.73718480e-011]\n",
      "toronto\t: [9.99999999e-001 1.71305116e-096 1.31624075e-201 5.58029291e-010]\n",
      "24\t: [9.99999998e-01 1.25814785e-43 1.37744595e-09 1.39059435e-10]\n",
      "boston\t: [1.00000000e+00 0.00000000e+00 0.00000000e+00 5.54066254e-94]\n",
      "________________________________\n",
      "lot\t: [0.07265716 0.73999993 0.17292914 0.01441378]\n",
      "nntp\t: [3.67122049e-01 4.58702066e-01 3.16568492e-24 1.74175885e-01]\n",
      "12\t: [2.78148863e-001 2.44029930e-150 5.48709224e-001 1.73141914e-001]\n",
      "bad\t: [1.60463387e-001 8.39536613e-001 1.46838198e-103 1.27759116e-012]\n",
      "sound\t: [0.02633051 0.56939531 0.31584142 0.08843277]\n",
      "best\t: [3.52270087e-01 3.18079734e-01 3.29650179e-01 5.36592124e-77]\n",
      "10\t: [0.31558947 0.13706024 0.39555366 0.15179663]\n",
      "20\t: [2.39002663e-01 6.30263091e-75 6.16161671e-01 1.44835666e-01]\n",
      "people\t: [0.01140045 0.28751872 0.69527519 0.00580564]\n",
      "society\t: [1.94868198e-81 5.41717029e-01 1.16260862e-01 3.42022110e-01]\n",
      "exist\t: [1.38764982e-02 9.53251294e-02 8.90798372e-01 1.31804692e-88]\n",
      "lie\t: [0.03709082 0.30144488 0.66146431 0.        ]\n",
      "mount\t: [0.05511649 0.8299725  0.11491101 0.        ]\n",
      "technical\t: [0.10223638 0.         0.         0.89776362]\n",
      "company\t: [7.88621755e-022 7.99053318e-001 8.05361122e-121 2.00946682e-001]\n",
      "say\t: [0.05466785 0.47521253 0.44402059 0.02609903]\n",
      "email\t: [1.47960742e-01 4.50468918e-30 3.89449892e-01 4.62589366e-01]\n",
      "hear\t: [9.75545952e-002 5.87000215e-001 3.15445190e-001 2.60569636e-206]\n",
      "mark\t: [0.40792942 0.34083061 0.18963539 0.06160457]\n",
      "air\t: [6.42683690e-001 3.01398288e-308 2.70492213e-001 8.68240969e-002]\n",
      "wrong\t: [2.14436727e-002 5.09666780e-001 4.68889547e-001 1.53577598e-288]\n",
      "distribution\t: [2.42336548e-001 5.03321352e-001 2.53704935e-129 2.54342100e-001]\n",
      "usa\t: [1.44445924e-01 7.77487717e-01 1.32089636e-59 7.80663594e-02]\n",
      "mot\t: [1.39444129e-01 9.28893394e-53 8.60555871e-01 2.26615018e-54]\n",
      "oil\t: [4.30668310e-126 1.00000000e+000 4.12829272e-226 1.69825336e-182]\n",
      "know\t: [0.06934356 0.44623454 0.46299026 0.02143164]\n",
      "auto\t: [1.67108963e-01 8.32792365e-01 9.86713121e-05 3.44623464e-39]\n",
      "come\t: [0.11126    0.27006121 0.59666329 0.02201549]\n",
      "stand\t: [0.14994307 0.33122105 0.49496804 0.02386784]\n",
      "post\t: [0.25076364 0.40465819 0.1853123  0.15926587]\n",
      "existence\t: [0.         0.05698377 0.94301623 0.        ]\n",
      "1993apr15\t: [1.93747799e-001 8.06252201e-001 1.91637350e-010 7.45011109e-206]\n",
      "like\t: [0.06960391 0.55646667 0.32728649 0.04664292]\n",
      "reply\t: [0.23971789 0.28070639 0.34694901 0.13262671]\n",
      "research\t: [5.01524769e-07 2.15957067e-01 3.84873872e-01 3.99168560e-01]\n",
      "________________________________\n",
      "true\t: [2.58937344e-02 9.74106258e-01 7.02000993e-09 2.54230694e-10]\n",
      "17\t: [9.99999975e-01 2.49184152e-72 2.31481808e-08 1.40913290e-09]\n",
      "couple\t: [9.99948603e-01 5.13808533e-05 1.40073847e-08 2.09943535e-09]\n",
      "owner\t: [1.07252844e-001 8.92747156e-001 2.43945598e-302 5.67743117e-016]\n",
      "problem\t: [6.71654417e-02 9.32834556e-01 2.05014596e-09 9.56363948e-27]\n",
      "story\t: [8.15035099e-08 9.99999912e-01 6.64131989e-09 0.00000000e+00]\n",
      "hour\t: [9.99999999e-01 5.13142304e-33 0.00000000e+00 8.00981643e-10]\n",
      "drive\t: [9.99999995e-001 8.72718507e-021 4.88129993e-009 6.23692870e-183]\n",
      "phone\t: [9.99999986e-001 7.38889378e-127 1.17860251e-008 2.65683231e-009]\n",
      "way\t: [1.06506289e-01 8.93493706e-01 5.16644441e-09 1.30851034e-20]\n",
      "price\t: [5.70777200e-02 9.42922280e-01 0.00000000e+00 1.18303506e-10]\n",
      "car\t: [2.53790043e-022 1.00000000e+000 2.20744763e-258 2.00036195e-102]\n",
      "change\t: [7.75051899e-02 9.22494810e-01 1.36386696e-13 4.70384341e-10]\n",
      "friend\t: [4.78906694e-02 9.52109331e-01 1.88370978e-19 1.12523938e-28]\n",
      "bring\t: [1.44017329e-001 8.55982656e-001 1.48387816e-008 4.40496548e-102]\n",
      "necessary\t: [0.00000000e+00 9.99999992e-01 7.89241258e-09 4.12822527e-10]\n",
      "tell\t: [1.82011394e-01 8.17988588e-01 1.77058255e-08 2.76011150e-10]\n",
      "time\t: [1.83616663e-01 8.16383331e-01 5.43454126e-09 3.88254982e-10]\n",
      "great\t: [3.13782440e-01 6.86217550e-01 9.70630893e-09 4.19288552e-10]\n",
      "think\t: [1.86300278e-01 8.13699715e-01 7.24330607e-09 1.64887674e-10]\n",
      "toronto\t: [9.99999996e-001 5.51473354e-095 9.04140628e-201 3.83321986e-009]\n",
      "80\t: [1.00000000e+00 6.60076463e-17 8.34378053e-28 0.00000000e+00]\n",
      "engine\t: [3.68459331e-054 9.99999999e-001 2.22335564e-258 6.05130830e-010]\n",
      "dealer\t: [1.93877801e-53 1.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "look\t: [2.13951657e-01 7.86048341e-01 1.70688416e-09 1.57048068e-10]\n",
      "ca\t: [6.95573316e-01 3.04426681e-01 2.00099371e-09 3.41434013e-10]\n",
      "cover\t: [9.99999984e-01 0.00000000e+00 8.59626832e-09 7.08582597e-09]\n",
      "oil\t: [4.71716407e-126 1.00000000e+000 4.71945784e-234 1.10285894e-190]\n",
      "local\t: [9.99999987e-01 3.15096876e-96 1.30330239e-29 1.33496940e-08]\n",
      "come\t: [3.10937698e-01 6.89062284e-01 1.74039254e-08 3.64789378e-10]\n",
      "________________________________\n",
      "jesus\t: [0.0000000e+00 1.1021043e-12 1.0000000e+00 0.0000000e+00]\n",
      "look\t: [2.29034838e-08 7.05504826e-01 2.94495151e-01 1.05115048e-10]\n",
      "true\t: [1.32915051e-09 4.19228352e-01 5.80771647e-01 8.15931793e-11]\n",
      "statement\t: [1.03570000e-223 4.35044383e-001 5.64955617e-001 1.61740533e-077]\n",
      "let\t: [3.38483254e-08 3.85347017e-01 6.14652949e-01 1.58646063e-24]\n",
      "10\t: [4.09693331e-08 1.36199236e-01 8.63800723e-01 7.30506940e-10]\n",
      "auto\t: [2.62073531e-08 9.99739667e-01 2.60307143e-04 2.00352183e-47]\n",
      "universe\t: [0.00000000e+000 1.86500573e-118 1.00000000e+000 3.14380959e-010]\n",
      "absolute\t: [0.00000000e+00 1.00000000e+00 9.29888818e-34 0.00000000e+00]\n",
      "discuss\t: [0.00000000e+000 4.20129351e-114 1.00000000e+000 6.85002920e-011]\n",
      "man\t: [3.34927397e-09 2.76580911e-01 7.23419085e-01 3.82441334e-11]\n",
      "ohio\t: [5.32750791e-221 1.00000000e+000 6.73908546e-110 2.40400017e-010]\n",
      "fact\t: [0.00000000e+00 2.63558253e-01 7.36441747e-01 6.07656122e-11]\n",
      "apr\t: [2.40151792e-09 2.79282878e-50 9.99999997e-01 1.17721945e-10]\n",
      "word\t: [3.23084382e-165 4.86877774e-002 9.51312223e-001 3.76042771e-011]\n",
      "christian\t: [0.         0.04989233 0.95010767 0.        ]\n",
      "world\t: [4.66516727e-09 5.16387473e-02 9.48361248e-01 3.94731587e-10]\n",
      "people\t: [8.20375786e-10 1.58373893e-01 8.41626106e-01 1.54869807e-11]\n",
      "student\t: [0.00000000e+00 9.99999998e-01 4.10064905e-48 1.59335302e-09]\n",
      "29\t: [3.98057500e-08 9.55399813e-01 4.46001459e-02 1.06652063e-09]\n",
      "interpretation\t: [0.0000000e+00 1.6771342e-83 1.0000000e+00 0.0000000e+00]\n",
      "yes\t: [5.20343713e-009 4.90157463e-001 5.09842531e-001 1.03342307e-213]\n",
      "god\t: [6.92561047e-237 1.11240909e-001 8.88759091e-001 0.00000000e+000]\n",
      "question\t: [8.00992851e-09 2.27197582e-02 9.77280234e-01 2.06793566e-10]\n",
      "sin\t: [0.00000000e+00 2.25129284e-25 1.00000000e+00 0.00000000e+00]\n",
      "believe\t: [1.50397745e-009 3.77618903e-001 6.22381095e-001 5.96835052e-231]\n",
      "jewish\t: [0.00000000e+000 8.41701573e-183 1.00000000e+000 0.00000000e+000]\n",
      "claim\t: [2.84233542e-009 6.68797227e-002 9.33120274e-001 3.06441553e-133]\n",
      "non\t: [1.75543919e-163 2.89540310e-001 7.10459690e-001 2.66922517e-010]\n",
      "evidence\t: [5.70833144e-09 3.22833206e-02 9.67716674e-01 1.44779348e-59]\n",
      "1993\t: [1.16108935e-08 2.03885087e-34 9.99999988e-01 4.11966915e-10]\n",
      "explain\t: [1.86350011e-94 1.00000000e+00 1.32036367e-14 1.08179981e-10]\n",
      "term\t: [2.63425984e-09 1.49921467e-01 8.50078530e-01 3.99052367e-10]\n",
      "group\t: [2.07022950e-93 5.58151870e-02 9.44184813e-01 4.05789725e-10]\n",
      "jr\t: [2.81141667e-08 5.28237912e-01 4.71762059e-01 2.21225517e-10]\n",
      "athos\t: [0.00000000e+000 4.29894049e-070 1.00000000e+000 1.43538095e-244]\n",
      "mention\t: [1.46248413e-08 9.99999985e-01 1.37728936e-19 3.11911383e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salvation\t: [0.00000000e+00 3.63081904e-20 1.00000000e+00 0.00000000e+00]\n",
      "scripture\t: [0.         0.04123028 0.95876972 0.        ]\n",
      "32\t: [2.87718720e-08 1.10217720e-29 9.99999971e-01 3.98614825e-10]\n",
      "reason\t: [1.27337423e-09 3.76976208e-01 6.23023790e-01 1.00697229e-10]\n",
      "ad\t: [0.00000000e+000 1.00000000e+000 7.63927442e-221 1.20200078e-010]\n",
      "department\t: [4.09143246e-008 1.23744550e-137 9.99999958e-001 1.00015219e-009]\n",
      "hold\t: [3.75800069e-08 7.02833699e-01 2.97166263e-01 6.81834721e-10]\n",
      "answer\t: [2.31692437e-165 5.33486820e-048 1.00000000e+000 1.99460928e-010]\n",
      "reserve\t: [7.47825857e-08 9.11141766e-04 9.99088780e-01 3.15355267e-09]\n",
      "need\t: [1.04224905e-08 1.85095454e-01 8.14904536e-01 1.38515691e-10]\n",
      "usa\t: [2.42708490e-08 9.99999975e-01 3.73353467e-59 4.86260985e-10]\n",
      "truth\t: [2.22859682e-71 8.49647137e-02 9.15035286e-01 0.00000000e+00]\n",
      "miss\t: [1.69801342e-08 7.83812238e-56 9.99999982e-01 5.47555745e-10]\n",
      "christianity\t: [0.00000000e+000 3.23622922e-002 9.67637708e-001 1.91694254e-206]\n",
      "rutgers\t: [0.00000000e+000 3.48540093e-068 1.00000000e+000 1.39130633e-275]\n",
      "cwru\t: [2.29141836e-08 9.99949627e-01 5.03497131e-05 5.17356201e-10]\n",
      "05\t: [7.06219628e-09 8.12124299e-73 9.99999992e-01 1.41471432e-09]\n",
      "physic\t: [1.44685034e-08 7.19664031e-61 9.99999983e-01 2.70774510e-09]\n",
      "like\t: [7.12781585e-09 4.36203107e-01 5.63796886e-01 1.77065680e-10]\n",
      "case\t: [2.94231177e-10 4.85263810e-01 5.14736189e-01 3.69875106e-10]\n",
      "western\t: [9.72112830e-09 6.70753132e-01 3.29246858e-01 4.43842026e-10]\n",
      "necessarily\t: [1.31348376e-09 2.90952803e-01 7.09047196e-01 5.41982779e-27]\n",
      "follow\t: [8.02398361e-09 2.33113112e-01 7.66886880e-01 1.31699463e-10]\n",
      "way\t: [6.73318228e-09 4.73588835e-01 5.26411158e-01 5.17212102e-21]\n",
      "56\t: [1.86005685e-007 1.44753711e-105 9.99999808e-001 5.54841831e-009]\n",
      "point\t: [4.70112580e-08 4.90654643e-01 5.09345309e-01 2.15063923e-10]\n",
      "________________________________\n",
      "limit\t: [7.77452002e-002 9.22254799e-001 1.07772955e-009 9.63581604e-107]\n",
      "mail\t: [4.44235099e-01 5.55764899e-01 6.12647665e-19 1.47876998e-09]\n",
      "11\t: [9.80944504e-01 1.90554855e-02 8.97364297e-09 1.42187114e-09]\n",
      "91\t: [9.99999999e-001 1.83379043e-200 0.00000000e+000 6.51043192e-010]\n",
      "major\t: [7.89303962e-01 2.10696030e-01 4.96019488e-09 3.48472505e-09]\n",
      "reserve\t: [9.98942958e-01 1.05703182e-03 5.77780717e-09 4.70111681e-09]\n",
      "sell\t: [1.31559388e-001 8.68440612e-001 6.45303162e-109 1.50007070e-049]\n",
      "engine\t: [5.06014343e-054 9.99999999e-001 2.13053739e-258 5.79868730e-010]\n",
      "problem\t: [8.99836700e-02 9.10016328e-01 1.91650327e-09 8.94021915e-27]\n",
      "trade\t: [9.99999999e-01 2.12972148e-10 8.14104628e-10 1.47145751e-10]\n",
      "western\t: [1.43010401e-01 8.56989596e-01 2.09696575e-09 7.28685473e-10]\n",
      "state\t: [2.70910822e-001 7.29089177e-001 1.43227197e-164 9.61607221e-010]\n",
      "oh\t: [5.08109661e-01 4.91890335e-01 3.32454427e-09 9.86162942e-76]\n",
      "mind\t: [4.00244449e-01 5.99755522e-01 2.83523203e-08 1.19759394e-09]\n",
      "really\t: [1.39834971e-01 8.60165026e-01 3.08448365e-09 3.93868745e-46]\n",
      "model\t: [1.29656021e-97 9.99999997e-01 2.43198976e-09 9.63859348e-11]\n",
      "away\t: [7.14795171e-01 2.85204816e-01 1.27348539e-08 3.57940620e-10]\n",
      "500\t: [3.80790178e-01 6.19209822e-01 1.70416207e-18 1.79155923e-10]\n",
      "reply\t: [5.62281823e-01 4.37718170e-01 5.92665674e-09 1.28697812e-09]\n",
      "student\t: [0.00000000e+00 9.99999998e-01 2.04413446e-56 2.04743846e-09]\n",
      "institute\t: [9.99999960e-01 3.26982434e-71 2.97718910e-08 1.04639419e-08]\n",
      "mile\t: [2.32145942e-008 9.99999976e-001 4.73583683e-137 6.94297944e-010]\n",
      "dealer\t: [2.66257196e-53 1.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      "thing\t: [8.31190836e-02 9.16880911e-01 5.23558826e-09 5.98812879e-20]\n",
      "car\t: [3.48536165e-022 1.00000000e+000 2.11529350e-258 1.91685382e-102]\n",
      "deal\t: [2.16636560e-01 7.83363438e-01 2.22292546e-09 1.43155148e-11]\n",
      "thanks\t: [8.26615864e-02 9.17338412e-01 1.31681024e-09 5.86160303e-10]\n",
      "usa\t: [2.18421620e-01 7.81578379e-01 1.45462013e-67 4.88360828e-10]\n",
      "ca\t: [7.58329143e-01 2.41670855e-01 1.52218527e-09 2.59733972e-10]\n",
      "92\t: [9.99999999e-01 1.11619120e-18 1.09620505e-15 1.15849264e-09]\n",
      "owner\t: [1.41622411e-001 8.58377589e-001 2.24762113e-302 5.23096944e-016]\n",
      "cwru\t: [2.08769630e-01 7.91230369e-01 1.98599743e-13 5.26033415e-10]\n",
      "ask\t: [1.57797806e-01 8.42202182e-01 1.11022312e-08 3.37003583e-10]\n",
      "appreciate\t: [1.35855328e-042 1.00000000e+000 2.84300937e-113 5.27478328e-077]\n",
      "case\t: [6.93309894e-03 9.93066895e-01 5.25100172e-09 9.72645433e-10]\n",
      "freenet\t: [1.00000000e+000 6.05714043e-093 0.00000000e+000 2.08710316e-275]\n",
      "23\t: [8.45931595e-01 1.54068397e-01 6.39202619e-09 9.97730105e-10]\n",
      "power\t: [8.07224039e-01 1.92775955e-01 3.32312809e-09 1.91395627e-09]\n",
      "little\t: [5.87421325e-01 4.12578672e-01 2.26897013e-09 1.06267011e-10]\n",
      "possible\t: [1.81975728e-95 9.99999968e-01 2.49469609e-08 7.43477484e-09]\n",
      "post\t: [4.82442857e-01 5.17557139e-01 2.59642672e-09 1.26762439e-09]\n",
      "17\t: [9.99999983e-01 1.81445897e-72 1.61519078e-08 9.83239009e-10]\n",
      "buy\t: [6.69845551e-002 9.33015445e-001 1.36270275e-178 3.75632413e-243]\n",
      "nntp\t: [5.46259299e-01 4.53740700e-01 3.43041434e-32 1.07216829e-09]\n",
      "________________________________\n",
      "contact\t: [0.96374799 0.         0.         0.03625201]\n",
      "score\t: [1.00000000e+000 6.73724137e-113 4.07915420e-010 0.00000000e+000]\n",
      "team\t: [9.95098721e-001 7.89279326e-240 2.23523520e-275 4.90127870e-003]\n",
      "try\t: [6.77848492e-01 3.00166941e-01 4.82664407e-09 2.19845628e-02]\n",
      "jet\t: [9.55131340e-001 1.33214388e-211 0.00000000e+000 4.48686604e-002]\n",
      "1993apr15\t: [7.37855486e-001 2.62144514e-001 3.18664580e-018 3.79254176e-206]\n",
      "leave\t: [6.79624781e-01 2.83257472e-01 6.30534425e-09 3.71177402e-02]\n",
      "actually\t: [4.75300430e-01 4.21685640e-01 4.08807358e-08 1.03013888e-01]\n",
      "win\t: [9.89037802e-001 1.09621978e-002 2.10335978e-259 0.00000000e+000]\n",
      "hear\t: [6.60624470e-001 3.39375520e-001 9.32717100e-009 2.35865174e-206]\n",
      "open\t: [1.27205775e-01 7.07998664e-01 1.17351293e-08 1.64795549e-01]\n",
      "nntp\t: [8.54634398e-01 9.11666347e-02 3.21778255e-32 5.41989677e-02]\n",
      "internet\t: [8.26334138e-01 1.96810041e-03 9.01708701e-09 1.71697752e-01]\n",
      "late\t: [4.23406597e-01 4.10030126e-01 6.99506194e-62 1.66563277e-01]\n",
      "stage\t: [0.00000000e+000 0.00000000e+000 2.26151789e-311 1.00000000e+000]\n",
      "need\t: [8.02569232e-01 1.58970047e-01 1.62878990e-08 3.84607050e-02]\n",
      "think\t: [7.00088921e-01 2.85941545e-01 1.13870724e-08 1.39695226e-02]\n",
      "post\t: [8.17883386e-01 1.12680942e-01 2.63906546e-09 6.94356686e-02]\n",
      "canada\t: [9.28234741e-01 0.00000000e+00 3.51830698e-09 7.17652552e-02]\n",
      "guy\t: [1. 0. 0. 0.]\n",
      "info\t: [5.90249815e-43 6.94117208e-01 5.01915166e-60 3.05882792e-01]\n",
      "ca\t: [9.50575768e-01 3.89045084e-02 1.14399755e-09 1.05197224e-02]\n",
      "coach\t: [1.00000000e+000 0.00000000e+000 0.00000000e+000 6.83995918e-250]\n",
      "19\t: [9.63081361e-001 3.87956998e-101 8.66688789e-009 3.69186304e-002]\n",
      "24\t: [9.78918216e-01 3.70770939e-43 3.87487034e-09 2.10817800e-02]\n",
      "hope\t: [6.12256913e-01 2.75940715e-01 5.24596712e-08 1.11802320e-01]\n",
      "final\t: [0.89873563 0.06195961 0.         0.03930476]\n",
      "cup\t: [1.00000000e+000 3.01568922e-120 4.63471313e-010 6.38098829e-138]\n",
      "winnipeg\t: [1.00000000e+000 4.22836654e-280 2.85858009e-161 1.01839603e-099]\n",
      "________________________________\n",
      "post\t: [2.31033252e-07 9.99999752e-01 1.11395745e-08 5.43857448e-09]\n",
      "jason\t: [4.87926459e-07 9.99999477e-01 3.45762313e-08 0.00000000e+00]\n",
      "guess\t: [1.01852613e-07 9.99999897e-01 2.02492918e-13 8.10509253e-10]\n",
      "nntp\t: [2.98385694e-07 9.99999696e-01 1.67876402e-31 5.24696294e-09]\n",
      "reply\t: [3.18380410e-07 9.99999645e-01 3.00653309e-08 6.52873948e-09]\n",
      "best\t: [4.12893393e-07 9.99999562e-01 2.52098302e-08 2.33108926e-84]\n",
      "make\t: [1.52531893e-07 9.99999822e-01 2.40055419e-08 1.79878379e-09]\n",
      "say\t: [4.28886531e-08 9.99999934e-01 2.27283544e-08 7.58904854e-10]\n",
      "ad\t: [0.00000000e+000 1.00000000e+000 8.45590142e-229 3.42970291e-010]\n",
      "research\t: [8.65811385e-13 9.99999931e-01 4.33514644e-08 2.55411046e-08]\n",
      "communication\t: [1.14250608e-206 9.99999993e-001 7.38038221e-010 5.95625392e-009]\n",
      "car\t: [8.63843104e-029 1.00000000e+000 4.69701099e-258 4.25639475e-102]\n",
      "wind\t: [4.38864200e-052 9.99999997e-001 1.28060785e-182 2.93031581e-009]\n",
      "________________________________\n",
      "56\t: [9.85585232e-001 2.14472000e-113 1.81143018e-010 1.44147682e-002]\n",
      "19\t: [9.94814531e-001 1.00467849e-107 1.17909473e-009 5.18546790e-003]\n",
      "35\t: [9.95372236e-01 6.99527984e-11 2.01736780e-10 4.62776421e-03]\n",
      "26\t: [9.96604480e-01 1.24842519e-08 1.49369780e-10 3.39550729e-03]\n",
      "second\t: [9.94046232e-01 1.92010503e-08 5.53624049e-10 5.95374815e-03]\n",
      "st\t: [9.98631584e-001 1.01084782e-007 1.27689382e-256 1.36831530e-003]\n",
      "40\t: [9.94067603e-01 2.94820318e-08 2.94098831e-23 5.93236714e-03]\n",
      "11\t: [9.93671968e-01 6.21483570e-10 7.17798175e-10 6.32803092e-03]\n",
      "33\t: [9.99999960e-01 4.04524211e-08 9.93865186e-87 9.95247786e-53]\n",
      "1993\t: [9.82900755e-01 4.82616777e-41 2.89399474e-09 1.70992422e-02]\n",
      "scorer\t: [1.00000000e+00 0.00000000e+00 0.00000000e+00 2.48878077e-63]\n",
      "12\t: [9.88812444e-001 1.85686115e-157 1.12177144e-009 1.11875545e-002]\n",
      "32\t: [9.93252922e-01 1.06393741e-36 1.18017234e-09 6.74707711e-03]\n",
      "jupiter\t: [6.31153622e-282 1.08537656e-269 0.00000000e+000 1.00000000e+000]\n",
      "00\t: [9.92900167e-01 8.83234360e-35 1.36444416e-09 7.09983151e-03]\n",
      "16\t: [9.95837689e-01 2.08488471e-08 1.05319663e-09 4.16228895e-03]\n",
      "quebec\t: [1. 0. 0. 0.]\n",
      "boston\t: [1.00000000e+00 0.00000000e+00 0.00000000e+00 1.16677021e-86]\n",
      "april\t: [9.58306127e-001 3.64180238e-136 3.96440098e-196 4.16938733e-002]\n",
      "18\t: [9.92525151e-01 6.43705295e-42 4.46226951e-10 7.47484813e-03]\n",
      "01\t: [8.80826066e-001 3.39290555e-105 7.44045450e-009 1.19173927e-001]\n",
      "48\t: [1.00000000e+00 0.00000000e+00 1.51273851e-10 0.00000000e+00]\n",
      "lemieux\t: [1. 0. 0. 0.]\n",
      "chicago\t: [1.00000000e+000 3.27183684e-151 4.03396992e-010 0.00000000e+000]\n",
      "washington\t: [9.85342172e-001 1.29226460e-090 8.18317951e-248 1.46578282e-002]\n",
      "period\t: [9.98803306e-01 6.36125622e-45 4.87993357e-10 1.19669354e-03]\n",
      "angeles\t: [9.88648557e-001 4.77630813e-128 7.79070073e-160 1.13514428e-002]\n",
      "buffalo\t: [1. 0. 0. 0.]\n",
      "louis\t: [9.99999942e-01 5.73037383e-08 1.17734195e-09 0.00000000e+00]\n",
      "los\t: [9.88384083e-001 7.44750942e-055 2.29208239e-197 1.16159174e-002]\n",
      "power\t: [9.89690269e-01 7.60972761e-09 3.21726814e-10 1.03097226e-02]\n",
      "total\t: [9.94450281e-01 1.71012156e-13 3.28221109e-10 5.54971865e-03]\n",
      "58\t: [1.00000000e+00 3.12559983e-42 1.91131005e-21 0.00000000e+00]\n",
      "10\t: [9.91333253e-01 9.21527661e-09 7.14542485e-10 8.66673707e-03]\n",
      "driver\t: [9.95609672e-01 1.66700841e-07 0.00000000e+00 4.39016144e-03]\n",
      "new\t: [9.78495813e-01 2.39460323e-08 3.02782018e-09 2.15041600e-02]\n",
      "special\t: [9.80570351e-01 0.00000000e+00 8.86244714e-10 1.94296480e-02]\n",
      "sun\t: [9.84452565e-01 4.74271798e-08 1.18108402e-09 1.55473859e-02]\n",
      "15\t: [9.88536954e-01 5.72482649e-09 8.86124361e-10 1.14630389e-02]\n",
      "24\t: [9.97080192e-01 9.46790164e-50 5.19813113e-10 2.91980724e-03]\n",
      "play\t: [1.00000000e+000 9.95796331e-173 1.48219694e-322 9.44826475e-124]\n",
      "ramsey\t: [1.00000000e+000 1.47913359e-271 0.00000000e+000 0.00000000e+000]\n",
      "goal\t: [9.99298921e-01 0.00000000e+00 0.00000000e+00 7.01078692e-04]\n",
      "game\t: [1.00000000e+000 8.59012975e-130 3.62336042e-011 8.03903049e-038]\n",
      "51\t: [9.99999999e-01 1.05371176e-61 8.64422077e-10 1.65976937e-62]\n",
      "44\t: [9.95189867e-01 1.23104041e-25 2.09813004e-10 4.81013329e-03]\n",
      "23\t: [9.94844824e-01 5.83370814e-09 5.93599612e-10 5.15516927e-03]\n",
      "ca\t: [9.98497444e-01 1.02452963e-08 1.58267480e-10 1.50254581e-03]\n",
      "calgary\t: [9.99999978e-001 2.15746346e-008 4.10486224e-106 1.71725530e-164]\n",
      "nhl\t: [1.00000000e+000 0.00000000e+000 5.51707999e-290 6.34676130e-240]\n",
      "summary\t: [9.98175135e-001 9.34769809e-113 1.52528180e-010 1.82486453e-003]\n",
      "pp\t: [1. 0. 0. 0.]\n",
      "brown\t: [1.00000000e+000 6.00627564e-043 0.00000000e+000 1.25036203e-136]\n",
      "55\t: [1.00000000e+000 1.28734018e-195 0.00000000e+000 1.96842353e-140]\n",
      "montreal\t: [1. 0. 0. 0.]\n",
      "25\t: [9.87670387e-01 3.11941377e-08 5.63826256e-10 1.23295811e-02]\n",
      "13\t: [9.96860115e-01 4.86451231e-09 9.40623843e-10 3.13987947e-03]\n",
      "pittsburgh\t: [9.99999984e-01 1.55139692e-08 0.00000000e+00 0.00000000e+00]\n",
      "17\t: [9.95698728e-01 5.81682697e-80 1.26995037e-09 4.30127108e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\t: [9.89105389e-01 5.58291997e-82 1.46642544e-09 1.08946094e-02]\n",
      "14\t: [9.94053340e-01 4.41556871e-09 1.35332930e-09 5.94665392e-03]\n",
      "29\t: [9.87033344e-01 6.62436252e-08 3.78073299e-11 1.29665897e-02]\n",
      "result\t: [5.63275858e-86 1.83123292e-19 1.40814171e-07 9.99999859e-01]\n",
      "31\t: [9.97734558e-01 3.12201899e-08 8.02883830e-10 2.26540957e-03]\n",
      "pt\t: [9.99999999e-01 0.00000000e+00 9.04049705e-10 0.00000000e+00]\n",
      "________________________________\n",
      "express\t: [1.38283343e-01 2.80163698e-07 9.22004818e-02 7.69515895e-01]\n",
      "sci\t: [3.03525034e-184 2.82496631e-212 9.31548608e-003 9.90684514e-001]\n",
      "pat\t: [1.57431900e-01 2.14799301e-37 9.31089045e-03 8.33257210e-01]\n",
      "space\t: [3.86551157e-238 7.16027042e-122 1.63942958e-056 1.00000000e+000]\n",
      "maybe\t: [7.66093043e-01 6.36703932e-06 2.33900589e-01 1.25543319e-09]\n",
      "digex\t: [5.21723405e-145 5.59210102e-298 4.50065509e-233 1.00000000e+000]\n",
      "usa\t: [3.78793022e-01 1.44506134e-06 4.11281647e-60 6.21205533e-01]\n",
      "actually\t: [1.21081214e-01 8.91780787e-07 2.83192709e-01 5.95725185e-01]\n",
      "post\t: [3.31678116e-01 3.79346989e-07 2.91025067e-02 6.39218998e-01]\n",
      "ti\t: [1.00000000e+00 8.85035135e-79 0.00000000e+00 5.23918090e-52]\n",
      "access\t: [8.86990980e-002 1.05764170e-106 7.15154502e-003 9.04149357e-001]\n",
      "real\t: [1.74381981e-01 1.28905482e-06 3.15025372e-01 5.10591358e-01]\n",
      "tell\t: [3.46890071e-01 1.21024882e-06 3.83883829e-01 2.69224890e-01]\n",
      "net\t: [2.19056001e-01 1.12791117e-27 4.28650365e-02 7.38078962e-01]\n",
      "communication\t: [2.33651355e-200 5.40386144e-007 2.74668028e-003 9.97252779e-001]\n",
      "dseg\t: [1.00000000e+00 1.34866328e-78 0.00000000e+00 4.62385531e-51]\n",
      "nntp\t: [4.09897306e-01 3.62987239e-07 4.19668324e-25 5.90102331e-01]\n",
      "read\t: [2.21282236e-01 3.27145967e-06 7.78714493e-01 1.00261086e-16]\n",
      "________________________________\n"
     ]
    }
   ],
   "source": [
    "#14-24 gives a good mix, but try whatever you like\n",
    "dStart = 0\n",
    "dEnd = 10\n",
    "\n",
    "\n",
    "def getWordsFromMatrix(WdTest):\n",
    "    originalWords  = np.array(vectorizer.get_feature_names())[WdTest] \n",
    "    return originalWords\n",
    "\n",
    "for dk in range(dStart,dEnd):\n",
    "    \n",
    "    origWords = getWordsFromMatrix(WdTest[dk])\n",
    "    wordMixtures = [origWords[n] + \"\\t: \" + str(phi[dk][n]) for n in range(len(phi[dk]))]\n",
    "    for wm in set(wordMixtures):\n",
    "        print(wm)\n",
    "    print(\"________________________________\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Objectives\n",
    "\n",
    "Well done! You have now implemented LDA, approximated the necessary variational parameters, and examined the results to infer information about topics in documents. If you feel like you would like to experiment some more, there are some variants that you could try:\n",
    "\n",
    "1. Load the provided dataset from the Associated Press docs dataset. This has random news articles from an undisclosed number of topics. Replace the dataset code in the beginning with what is provided in the next cell and redo your tests. What kind of topics does your result have? How many topics did you assume there were? (Some interesting cases I got were general topics like Crime and Economics and then one focusing solely on foreign affairs with President Bush)\n",
    "\n",
    "2. Run the tests using the MoodyLyrics dataset instead. This dataset includes the lyrics from songs in many different genres (I've included has slightly less than 200 / 50 documents and V=500). Run the tests again and see what kind of sense LDA tries to make out of these song lyrics. The dataset also provides an annotation as to what emotion (\"Angry\", \"Sad\", \"Happy\", \"Relaxed\") the song exhibits. Can you find a resemblence in your topics to these emotions? (<i>Disclaimer: The lyrics provided are not censored and some are not exactly \"PG-13\")</i>\n",
    "\n",
    "<b>It is possible to start to see results after ~50-60 iterations so if you would like to try out these bonus exercises you need not wait overnight</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the AP docs dataset instead:\n",
    "#(everything else should work like before)\n",
    "'''vectorizer = pickle.load(open(\"Data/vectorizerAP.p\", \"rb\"), encoding='latin1')\n",
    "trainDocs = pickle.load(open(\"Data/trainDocsAP.p\", \"rb\"), encoding='latin1')\n",
    "testDocs = pickle.load(open(\"Data/testDocsAP.p\", \"rb\"), encoding='latin1')\n",
    "\n",
    "test1TrainDocs = pickle.load(open(\"Data/trainDocsAP.p\", \"rb\"), encoding='latin1')\n",
    "test1TestDocs = pickle.load(open(\"Data/testDocsAP.p\", \"rb\"), encoding='latin1')\n",
    "\n",
    "#loading the moodyLyrics dataset instead:\n",
    "vectorizer = pickle.load(open(\"Data/vectorizerMoodyLyrics.p\", \"rb\"), encoding='latin1')\n",
    "trainLyricsFile = pickle.load(open(\"Data/trainDocsMoodyLyrics.p\", \"rb\"), encoding='latin1')\n",
    "testLyricsFile = pickle.load(open(\"Data/testDocsMoodyLyrics.p\", \"rb\"), encoding='latin1')\n",
    "\n",
    "trainDocs = trainLyricsFile['lyrics']\n",
    "testDocs = testLyricsFile['lyrics']\n",
    "trainGT = trainLyricsFile['groundTruth']\n",
    "#original moods can be seen with: trainGT = trainLyricsFile['groundTruth'] but the labeling is not perfect. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
